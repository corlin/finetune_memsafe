#!/usr/bin/env python3
"""
Qwen3-4B-Thinking-2507 ä¸“ç”¨å¯åŠ¨è„šæœ¬

é’ˆå¯¹Qwen3-4B-Thinking-2507æ¨¡å‹ä¼˜åŒ–çš„å¿«é€Ÿå¯åŠ¨è„šæœ¬ã€‚
"""

import subprocess
import sys
import json
from pathlib import Path


def check_requirements():
    """æ£€æŸ¥åŸºæœ¬è¦æ±‚"""
    print("=== æ£€æŸ¥ç³»ç»Ÿè¦æ±‚ ===")
    
    # æ£€æŸ¥uv
    try:
        result = subprocess.run(["uv", "--version"], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ“ uvå·²å®‰è£…: {result.stdout.strip()}")
        else:
            print("âœ— uvæœªæ­£ç¡®å®‰è£…")
            return False
    except FileNotFoundError:
        print("âœ— uvæœªå®‰è£…")
        print("è¯·å®‰è£…uv: curl -LsSf https://astral.sh/uv/install.sh | sh")
        return False
    
    # æ£€æŸ¥GPU
    try:
        result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ“ NVIDIA GPUå¯ç”¨")
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
    except FileNotFoundError:
        print("âš ï¸  nvidia-smiæœªæ‰¾åˆ°ï¼Œå¯èƒ½æ²¡æœ‰NVIDIA GPU")
    
    return True


def sync_dependencies():
    """åŒæ­¥ä¾èµ–"""
    print("\n=== åŒæ­¥ä¾èµ– ===")
    try:
        result = subprocess.run(["uv", "sync"], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ“ ä¾èµ–åŒæ­¥å®Œæˆ")
            return True
        else:
            print(f"âœ— ä¾èµ–åŒæ­¥å¤±è´¥: {result.stderr}")
            return False
    except Exception as e:
        print(f"âœ— ä¾èµ–åŒæ­¥å¼‚å¸¸: {e}")
        return False


def run_qwen3_thinking_finetuning(mode="default"):
    """è¿è¡ŒQwen3-4B-Thinking-2507å¾®è°ƒ"""
    print(f"\n=== å¼€å§‹Qwen3-4B-Thinking-2507å¾®è°ƒ ({mode}æ¨¡å¼) ===")
    
    if mode == "quick_test":
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        cmd = [
            "uv", "run", "main.py",
            "--model-name", "Qwen/Qwen3-4B-Thinking-2507",
            "--output-dir", "./qwen3-4b-thinking-test",
            "--max-memory-gb", "10",
            "--batch-size", "2",
            "--gradient-accumulation-steps", "32",
            "--num-epochs", "5",
            "--max-sequence-length", "128",
            "--auto-install-deps"
        ]
    elif mode == "low_memory":
        # ä½æ˜¾å­˜æ¨¡å¼ (6-8GB GPU)
        cmd = [
            "uv", "run", "main.py",
            "--model-name", "Qwen/Qwen3-4B-Thinking-2507",
            "--output-dir", "./qwen3-4b-thinking-low-mem",
            "--max-memory-gb", "6",
            "--batch-size", "1",
            "--gradient-accumulation-steps", "64",
            "--num-epochs", "30",
            "--max-sequence-length", "128",
            "--auto-install-deps"
        ]
    elif mode == "config_file":
        # ä½¿ç”¨é…ç½®æ–‡ä»¶æ¨¡å¼
        cmd = [
            "uv", "run", "main.py",
            "--config", "qwen3_4b_thinking_config.json"
        ]
    else:
        # é»˜è®¤æ¨¡å¼
        cmd = [
            "uv", "run", "main.py",
            "--model-name", "Qwen/Qwen3-4B-Thinking-2507",
            "--auto-install-deps"
        ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        # å®æ—¶æ˜¾ç¤ºè¾“å‡º
        process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            universal_newlines=True, 
            bufsize=1
        )
        
        for line in process.stdout:
            print(line, end='')
        
        process.wait()
        
        if process.returncode == 0:
            print("\nğŸ‰ Qwen3-4B-Thinking-2507å¾®è°ƒå®Œæˆï¼")
            return True
        else:
            print(f"\nâœ— å¾®è°ƒå¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}")
            return False
            
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­äº†å¾®è°ƒè¿‡ç¨‹")
        process.terminate()
        return False
    except Exception as e:
        print(f"\nâœ— è¿è¡Œå¼‚å¸¸: {e}")
        return False


def show_results():
    """æ˜¾ç¤ºç»“æœ"""
    print("\n=== å¾®è°ƒç»“æœ ===")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    possible_dirs = [
        "./qwen3-4b-thinking-finetuned",
        "./qwen3-4b-thinking-test", 
        "./qwen3-4b-thinking-low-mem",
        "./qwen3-finetuned"
    ]
    
    for output_dir in possible_dirs:
        if Path(output_dir).exists():
            print(f"âœ“ æ‰¾åˆ°å¾®è°ƒç»“æœ: {output_dir}")
            
            # åˆ—å‡ºä¸»è¦æ–‡ä»¶
            output_path = Path(output_dir)
            important_files = [
                "adapter_config.json",
                "adapter_model.safetensors", 
                "config.json",
                "tokenizer.json",
                "final_application_report.json"
            ]
            
            for file in important_files:
                if (output_path / file).exists():
                    print(f"  âœ“ {file}")
                else:
                    print(f"  âœ— {file} (ç¼ºå¤±)")
            
            # æä¾›åç»­æ“ä½œå»ºè®®
            print(f"\nåç»­æ“ä½œ:")
            print(f"1. æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Š: cat {output_dir}/final_application_report.json")
            print(f"2. å¯åŠ¨TensorBoard: uv run tensorboard --logdir {output_dir}/logs/tensorboard")
            print(f"3. æµ‹è¯•æ¨ç†:")
            print(f'   uv run python -c "from src.inference_tester import InferenceTester; tester = InferenceTester(); tester.load_finetuned_model(\'{output_dir}\', \'Qwen/Qwen3-4B-Thinking-2507\'); print(tester.test_inference(\'è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\'))"')
            
            break
    else:
        print("âœ— æœªæ‰¾åˆ°å¾®è°ƒç»“æœç›®å½•")


def main():
    """ä¸»å‡½æ•°"""
    print("=== Qwen3-4B-Thinking-2507 ä¸“ç”¨å¾®è°ƒå¯åŠ¨å™¨ ===\n")
    
    # 1. æ£€æŸ¥è¦æ±‚
    if not check_requirements():
        print("ç³»ç»Ÿè¦æ±‚æ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³é—®é¢˜åé‡è¯•")
        sys.exit(1)
    
    # 2. åŒæ­¥ä¾èµ–
    if not sync_dependencies():
        print("ä¾èµ–åŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        sys.exit(1)
    
    # 3. é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\n=== é€‰æ‹©è¿è¡Œæ¨¡å¼ ===")
    print("1. å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (5è½®è®­ç»ƒï¼Œé€‚åˆéªŒè¯ç¯å¢ƒ)")
    print("2. ä½æ˜¾å­˜æ¨¡å¼ (é€‚åˆ6-8GB GPU)")
    print("3. æ ‡å‡†æ¨¡å¼ (æ¨èï¼Œé€‚åˆ10GB+GPU)")
    print("4. é…ç½®æ–‡ä»¶æ¨¡å¼ (ä½¿ç”¨qwen3_4b_thinking_config.json)")
    print("5. ç¯å¢ƒæ£€æŸ¥æ¨¡å¼ (ä»…æ£€æŸ¥ç¯å¢ƒï¼Œä¸è®­ç»ƒ)")
    
    choice = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1-5ï¼Œé»˜è®¤3): ").strip() or "3"
    
    if choice == "1":
        success = run_qwen3_thinking_finetuning("quick_test")
    elif choice == "2":
        success = run_qwen3_thinking_finetuning("low_memory")
    elif choice == "3":
        success = run_qwen3_thinking_finetuning("default")
    elif choice == "4":
        if not Path("qwen3_4b_thinking_config.json").exists():
            print("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼")
            success = run_qwen3_thinking_finetuning("default")
        else:
            success = run_qwen3_thinking_finetuning("config_file")
    elif choice == "5":
        print("è¿è¡Œç¯å¢ƒæ£€æŸ¥...")
        result = subprocess.run([
            "uv", "run", "python", "check_compatibility_2025.py"
        ])
        success = result.returncode == 0
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼")
        success = run_qwen3_thinking_finetuning("default")
    
    # 4. æ˜¾ç¤ºç»“æœ
    if success and choice != "5":
        show_results()
    elif not success:
        print("\næ•…éšœæ’é™¤å»ºè®®:")
        print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ (nvidia-smi)")
        print("2. å°è¯•ä½æ˜¾å­˜æ¨¡å¼")
        print("3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: logs/application.log")
        print("4. è¿è¡Œç¯å¢ƒæ£€æŸ¥: uv run python check_compatibility_2025.py")
        print("5. æ›´æ–°ä¾èµ–: uv run python update_dependencies.py")


if __name__ == "__main__":
    main()