#!/usr/bin/env python3
"""
ä½¿ç”¨uvç®¡ç†ä¾èµ–ï¼Œå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
"""

import os
import subprocess
import sys
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_command(cmd, cwd=None):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    try:
        logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, cwd=cwd, check=True, capture_output=True, text=True)
        if result.stdout:
            logger.info(f"è¾“å‡º: {result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        if e.stderr:
            logger.error(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
        return False

def setup_llama_cpp():
    """è®¾ç½®llama.cppç¯å¢ƒ"""
    llama_cpp_dir = "llama.cpp"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(llama_cpp_dir):
        logger.info("llama.cppç›®å½•å·²å­˜åœ¨")
        return llama_cpp_dir
    
    # å…‹éš†ä»“åº“
    logger.info("æ­£åœ¨å…‹éš†llama.cppä»“åº“...")
    if not run_command(["git", "clone", "https://github.com/ggerganov/llama.cpp.git"]):
        return None
    
    return llama_cpp_dir

def install_dependencies_with_uv():
    """ä½¿ç”¨uvå®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–"""
    logger.info("æ­£åœ¨ä½¿ç”¨uvå®‰è£…ä¾èµ–åŒ…...")
    
    # å¿…è¦çš„ä¾èµ–åŒ…
    dependencies = [
        "torch",
        "transformers", 
        "sentencepiece",
        "protobuf",
        "accelerate",
        "safetensors",
        "numpy",
        "huggingface-hub"
    ]
    
    # æ‰¹é‡å®‰è£…ä¾èµ–
    logger.info("æ‰¹é‡å®‰è£…ä¾èµ–åŒ…...")
    cmd = ["uv", "add"] + dependencies
    
    if run_command(cmd):
        logger.info("æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ")
        return True
    else:
        logger.warning("uvæ‰¹é‡å®‰è£…å¤±è´¥ï¼Œå°è¯•é€ä¸ªå®‰è£…...")
        
        # é€ä¸ªå®‰è£…
        success_count = 0
        for dep in dependencies:
            logger.info(f"å®‰è£… {dep}...")
            if run_command(["uv", "add", dep]):
                success_count += 1
                logger.info(f"âœ“ {dep} å®‰è£…æˆåŠŸ")
            else:
                logger.warning(f"âœ— {dep} å®‰è£…å¤±è´¥")
        
        logger.info(f"æˆåŠŸå®‰è£… {success_count}/{len(dependencies)} ä¸ªä¾èµ–åŒ…")
        return success_count > len(dependencies) // 2  # è‡³å°‘ä¸€åŠæˆåŠŸ

def check_model_files(model_path):
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
    logger.info("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    
    required_files = [
        "config.json",
        "tokenizer.json", 
        "tokenizer_config.json"
    ]
    
    safetensors_files = [
        "model-00001-of-00002.safetensors",
        "model-00002-of-00002.safetensors"
    ]
    
    # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if not os.path.exists(file_path):
            logger.error(f"ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {file}")
            return False
        logger.info(f"âœ“ æ‰¾åˆ°æ–‡ä»¶: {file}")
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
    found_weights = False
    for file in safetensors_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            logger.info(f"âœ“ æ‰¾åˆ°æƒé‡æ–‡ä»¶: {file}")
            found_weights = True
    
    if not found_weights:
        logger.error("æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶")
        return False
    
    logger.info("æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
    return True

def convert_model():
    """è½¬æ¢æ¨¡å‹"""
    # è·¯å¾„è®¾ç½®
    model_path = "exported_models/qwen3_merged_lightweight/Qwen_Qwen3-4B-Thinking-2507_pytorch_20250816_225620"
    output_dir = "exported_models/qwen3_merged_lightweight/gguf"
    output_file = os.path.join(output_dir, "qwen3-4b-thinking-f16.gguf")
    llama_cpp_dir = "llama.cpp"
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    if not os.path.exists(model_path):
        logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not check_model_files(model_path):
        logger.error("æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    
    # æ£€æŸ¥è½¬æ¢è„šæœ¬
    convert_script = os.path.join(llama_cpp_dir, "convert_hf_to_gguf.py")
    if not os.path.exists(convert_script):
        logger.error(f"è½¬æ¢è„šæœ¬ä¸å­˜åœ¨: {convert_script}")
        logger.info("è¯·ç¡®ä¿llama.cppå·²æ­£ç¡®å…‹éš†")
        return False
    
    # æ‰§è¡Œè½¬æ¢
    logger.info("="*60)
    logger.info("å¼€å§‹è½¬æ¢æ¨¡å‹...")
    logger.info(f"è¾“å…¥: {model_path}")
    logger.info(f"è¾“å‡º: {output_file}")
    logger.info("="*60)
    
    # ä½¿ç”¨uv runæ¥æ‰§è¡Œè½¬æ¢è„šæœ¬
    cmd = [
        "uv", "run", "python", convert_script,
        model_path,
        "--outfile", output_file,
        "--outtype", "f16"
    ]
    
    if run_command(cmd):
        logger.info("æ¨¡å‹è½¬æ¢æˆåŠŸ!")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / (1024 * 1024 * 1024)
            logger.info(f"GGUFæ–‡ä»¶å¤§å°: {file_size:.2f} GB")
            logger.info(f"è¾“å‡ºæ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}")
            return True
        else:
            logger.error("è½¬æ¢å®Œæˆä½†æ‰¾ä¸åˆ°è¾“å‡ºæ–‡ä»¶")
            return False
    else:
        logger.error("æ¨¡å‹è½¬æ¢å¤±è´¥")
        return False

def create_requirements_txt():
    """åˆ›å»ºrequirements.txtæ–‡ä»¶ä»¥å¤‡ç”¨"""
    requirements_content = """torch>=2.0.0
transformers>=4.30.0
sentencepiece
protobuf
accelerate
safetensors
numpy
huggingface-hub
"""
    
    with open("requirements.txt", "w") as f:
        f.write(requirements_content)
    
    logger.info("å·²åˆ›å»ºrequirements.txtæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*60)
    logger.info("å¼€å§‹PyTorchåˆ°GGUFçš„è½¬æ¢è¿‡ç¨‹ (ä½¿ç”¨uv)")
    logger.info("="*60)
    
    # 0. åˆ›å»ºrequirements.txtå¤‡ç”¨
    create_requirements_txt()
    
    # 1. è®¾ç½®llama.cpp
    logger.info("æ­¥éª¤ 1: è®¾ç½®llama.cppç¯å¢ƒ")
    llama_cpp_dir = setup_llama_cpp()
    if not llama_cpp_dir:
        logger.error("æ— æ³•è®¾ç½®llama.cppç¯å¢ƒ")
        return
    
    # 2. ä½¿ç”¨uvå®‰è£…ä¾èµ–
    logger.info("æ­¥éª¤ 2: ä½¿ç”¨uvå®‰è£…Pythonä¾èµ–")
    if not install_dependencies_with_uv():
        logger.error("ä¾èµ–å®‰è£…å¤±è´¥")
        logger.info("å°è¯•æ‰‹åŠ¨å®‰è£…: uv add torch transformers sentencepiece protobuf accelerate safetensors numpy")
        return
    
    # 3. è½¬æ¢æ¨¡å‹
    logger.info("æ­¥éª¤ 3: è½¬æ¢æ¨¡å‹")
    if convert_model():
        logger.info("="*60)
        logger.info("ğŸ‰ è½¬æ¢å®Œæˆ!")
        logger.info("GGUFæ¨¡å‹å·²ä¿å­˜åˆ°: exported_models/qwen3_merged_lightweight/gguf/")
        logger.info("æ–‡ä»¶å: qwen3-4b-thinking-f16.gguf")
        logger.info("="*60)
    else:
        logger.error("è½¬æ¢å¤±è´¥")
        logger.info("="*60)
        logger.info("æ‰‹åŠ¨è½¬æ¢æ­¥éª¤:")
        logger.info("1. ç¡®ä¿å·²å…‹éš†llama.cpp: git clone https://github.com/ggerganov/llama.cpp.git")
        logger.info("2. å®‰è£…ä¾èµ–: uv add torch transformers sentencepiece protobuf accelerate safetensors numpy")
        logger.info("3. è¿è¡Œè½¬æ¢:")
        logger.info("   uv run python llama.cpp/convert_hf_to_gguf.py \\")
        logger.info("   exported_models/qwen3_merged_lightweight/Qwen_Qwen3-4B-Thinking-2507_pytorch_20250816_225620 \\")
        logger.info("   --outfile exported_models/qwen3_merged_lightweight/gguf/qwen3-4b-thinking-f16.gguf \\")
        logger.info("   --outtype f16")
        logger.info("="*60)

if __name__ == "__main__":
    main()