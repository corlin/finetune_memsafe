"""
PyTorchæ ¼å¼å¯¼å‡ºçš„é›†æˆæµ‹è¯•

è¿™ä¸ªæµ‹è¯•æ¼”ç¤ºäº†å®Œæ•´çš„PyTorchæ¨¡å‹å¯¼å‡ºæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. é…ç½®ç®¡ç†
2. æ¨¡å‹åˆå¹¶
3. æ¨¡å‹ä¼˜åŒ–
4. PyTorchæ ¼å¼å¯¼å‡º
5. å¯¼å‡ºéªŒè¯
"""

import os
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch
import torch
import torch.nn as nn
import pytest

from src.export_config import ConfigurationManager
from src.export_models import ExportConfiguration, QuantizationLevel
from src.model_merger import ModelMerger
from src.optimization_processor import OptimizationProcessor
from src.format_exporter import FormatExporter


class MockQwenModel(nn.Module):
    """æ¨¡æ‹ŸQwenæ¨¡å‹ç”¨äºé›†æˆæµ‹è¯•"""
    
    def __init__(self):
        super().__init__()
        self.embed_tokens = nn.Embedding(32000, 4096)
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=4096, nhead=32, batch_first=True)
            for _ in range(2)  # ç®€åŒ–ä¸º2å±‚
        ])
        self.norm = nn.LayerNorm(4096)
        self.lm_head = nn.Linear(4096, 32000)
        
        # æ¨¡æ‹Ÿé…ç½®
        self.config = Mock()
        self.config.model_type = "qwen2"
        self.config.vocab_size = 32000
        self.config.hidden_size = 4096
        self.config.num_hidden_layers = 2
        self.config.num_attention_heads = 32
        self.config.to_dict.return_value = {
            'model_type': 'qwen2',
            'vocab_size': 32000,
            'hidden_size': 4096,
            'num_hidden_layers': 2,
            'num_attention_heads': 32,
            'architectures': ['Qwen2ForCausalLM']
        }
    
    def forward(self, input_ids, attention_mask=None):
        x = self.embed_tokens(input_ids)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        logits = self.lm_head(x)
        return Mock(logits=logits)
    
    def save_pretrained(self, path, **kwargs):
        """æ¨¡æ‹Ÿsave_pretrainedæ–¹æ³•"""
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        import json
        config_path = path / "config.json"
        with open(config_path, 'w') as f:
            json.dump(self.config.to_dict(), f, indent=2)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        model_path = path / "model.safetensors"
        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ¨¡æ‹Ÿæ–‡ä»¶
        model_path.write_bytes(b"mock_model_weights" * 10000)
        
        # ä¿å­˜pytorch_model.binä½œä¸ºå¤‡é€‰
        torch.save(self.state_dict(), path / "pytorch_model.bin")
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        super().eval()
        return self


class TestPyTorchExportIntegration:
    """PyTorchå¯¼å‡ºé›†æˆæµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæ¨¡æ‹Ÿcheckpointç›®å½•
        self.checkpoint_dir = os.path.join(self.temp_dir, "mock_checkpoint")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        self.config = ExportConfiguration(
            checkpoint_path=self.checkpoint_dir,
            base_model_name="Qwen/Qwen2-4B",
            output_directory=self.temp_dir,
            quantization_level=QuantizationLevel.NONE,
            export_pytorch=True,
            save_tokenizer=True,
            run_validation_tests=False  # è·³è¿‡éªŒè¯ä»¥ç®€åŒ–æµ‹è¯•
        )
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        self.mock_model = MockQwenModel()
        
    def teardown_method(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def create_mock_tokenizer(self):
        """åˆ›å»ºæ¨¡æ‹Ÿtokenizer"""
        tokenizer = Mock()
        tokenizer.save_pretrained = Mock()
        tokenizer.pad_token = None
        tokenizer.eos_token = "<|endoftext|>"
        return tokenizer
    
    @patch('src.format_exporter.ensure_disk_space')
    @patch('src.format_exporter.get_directory_size_mb')
    def test_complete_pytorch_export_workflow(self, mock_get_size, mock_ensure_disk):
        """æµ‹è¯•å®Œæ•´çš„PyTorchå¯¼å‡ºå·¥ä½œæµç¨‹"""
        mock_get_size.return_value = 150.0  # æ¨¡æ‹Ÿå¯¼å‡ºå¤§å°150MB
        
        print("\n=== PyTorchæ¨¡å‹å¯¼å‡ºé›†æˆæµ‹è¯• ===")
        
        # 1. é…ç½®ç®¡ç†æµ‹è¯•
        print("1. æµ‹è¯•é…ç½®ç®¡ç†...")
        config_manager = ConfigurationManager()
        
        # éªŒè¯é…ç½®
        validation_errors = config_manager.validate_configuration(self.config)
        assert len(validation_errors) == 0, f"é…ç½®éªŒè¯å¤±è´¥: {validation_errors}"
        print("   âœ“ é…ç½®éªŒè¯é€šè¿‡")
        
        # 2. æ¨¡å‹ä¼˜åŒ–æµ‹è¯•
        print("2. æµ‹è¯•æ¨¡å‹ä¼˜åŒ–...")
        optimizer = OptimizationProcessor(device="cpu")
        
        # åº”ç”¨ä¼˜åŒ–ï¼ˆè·³è¿‡é‡åŒ–ä»¥ç®€åŒ–æµ‹è¯•ï¼‰
        optimized_model = optimizer.remove_training_artifacts(self.mock_model)
        assert optimized_model is not None
        print("   âœ“ æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
        
        # è·å–ä¼˜åŒ–ç»Ÿè®¡
        optimization_report = optimizer.get_optimization_report()
        assert 'optimization_stats' in optimization_report
        print(f"   âœ“ ä¼˜åŒ–ç»Ÿè®¡: {len(optimization_report['optimization_stats']['optimization_steps'])} ä¸ªæ­¥éª¤")
        
        # 3. PyTorchæ ¼å¼å¯¼å‡ºæµ‹è¯•
        print("3. æµ‹è¯•PyTorchæ ¼å¼å¯¼å‡º...")
        exporter = FormatExporter(self.config)
        
        # åˆ›å»ºtokenizer
        tokenizer = self.create_mock_tokenizer()
        
        # æ‰§è¡Œå¯¼å‡º
        export_path = exporter.export_pytorch_model(optimized_model, tokenizer)
        
        # éªŒè¯å¯¼å‡ºç»“æœ
        assert os.path.exists(export_path)
        assert os.path.isdir(export_path)
        print(f"   âœ“ æ¨¡å‹å¯¼å‡ºåˆ°: {export_path}")
        
        # æ£€æŸ¥å¯¼å‡ºçš„æ–‡ä»¶
        export_path_obj = Path(export_path)
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        required_files = ['config.json', 'export_metadata.json']
        for file_name in required_files:
            file_path = export_path_obj / file_name
            assert file_path.exists(), f"ç¼ºå¤±æ–‡ä»¶: {file_name}"
            print(f"   âœ“ æ‰¾åˆ°æ–‡ä»¶: {file_name}")
        
        # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
        model_files = list(export_path_obj.glob('*.safetensors')) + list(export_path_obj.glob('*.bin'))
        assert len(model_files) > 0, "æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶"
        print(f"   âœ“ æ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶: {[f.name for f in model_files]}")
        
        # 4. éªŒè¯å¯¼å‡ºç»Ÿè®¡
        print("4. éªŒè¯å¯¼å‡ºç»Ÿè®¡...")
        export_stats = exporter.get_export_stats()
        
        assert export_stats['pytorch_export']['success'] is True
        assert export_stats['pytorch_export']['size_mb'] == 150.0
        assert export_stats['total_exports'] == 1
        assert export_stats['successful_exports'] == 1
        print("   âœ“ å¯¼å‡ºç»Ÿè®¡éªŒè¯é€šè¿‡")
        
        # 5. éªŒè¯å…ƒæ•°æ®
        print("5. éªŒè¯å¯¼å‡ºå…ƒæ•°æ®...")
        metadata_path = export_path_obj / 'export_metadata.json'
        
        import json
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # æ£€æŸ¥å…ƒæ•°æ®ç»“æ„
        assert 'export_info' in metadata
        assert 'model_info' in metadata
        assert metadata['export_info']['export_format'] == 'pytorch'
        assert metadata['export_info']['base_model'] == 'Qwen/Qwen2-4B'
        assert metadata['model_info']['model_type'] == 'qwen2'
        print("   âœ“ å…ƒæ•°æ®éªŒè¯é€šè¿‡")
        
        # 6. æµ‹è¯•ä½¿ç”¨ç¤ºä¾‹ç”Ÿæˆ
        print("6. æµ‹è¯•ä½¿ç”¨ç¤ºä¾‹ç”Ÿæˆ...")
        usage_example = exporter.generate_usage_example(export_path)
        
        assert 'AutoModelForCausalLM' in usage_example
        assert 'AutoTokenizer' in usage_example
        assert export_path in usage_example
        print("   âœ“ ä½¿ç”¨ç¤ºä¾‹ç”ŸæˆæˆåŠŸ")
        
        # 7. æµ‹è¯•éƒ¨ç½²åŒ…åˆ›å»º
        print("7. æµ‹è¯•éƒ¨ç½²åŒ…åˆ›å»º...")
        with patch('src.format_exporter.shutil.make_archive') as mock_make_archive:
            # æ¨¡æ‹ŸZIPæ–‡ä»¶åˆ›å»º
            def mock_make_archive_side_effect(base_name, format, root_dir):
                zip_path = Path(f"{base_name}.zip")
                zip_path.write_bytes(b"mock_deployment_package" * 1000)
                return base_name
            
            mock_make_archive.side_effect = mock_make_archive_side_effect
            
            package_path = exporter.create_deployment_package(export_path)
            assert package_path.endswith('.zip')
            print(f"   âœ“ éƒ¨ç½²åŒ…åˆ›å»ºæˆåŠŸ: {Path(package_path).name}")
        
        print("\n=== é›†æˆæµ‹è¯•å®Œæˆ ===")
        print(f"å¯¼å‡ºè·¯å¾„: {export_path}")
        print(f"å¯¼å‡ºå¤§å°: {export_stats['pytorch_export']['size_mb']} MB")
        print(f"ä¼˜åŒ–æ­¥éª¤: {len(optimization_report['optimization_stats']['optimization_steps'])}")
        print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    
    def test_configuration_file_workflow(self):
        """æµ‹è¯•é…ç½®æ–‡ä»¶å·¥ä½œæµç¨‹"""
        print("\n=== é…ç½®æ–‡ä»¶å·¥ä½œæµç¨‹æµ‹è¯• ===")
        
        # 1. åˆ›å»ºé…ç½®æ¨¡æ¿
        config_manager = ConfigurationManager()
        template_path = os.path.join(self.temp_dir, "export_config_template.yaml")
        
        config_manager.create_config_template(template_path)
        assert os.path.exists(template_path)
        print(f"   âœ“ é…ç½®æ¨¡æ¿åˆ›å»º: {template_path}")
        
        # 2. ä¿å­˜å½“å‰é…ç½®
        config_path = os.path.join(self.temp_dir, "current_config.yaml")
        config_manager.save_configuration(self.config, config_path)
        assert os.path.exists(config_path)
        print(f"   âœ“ é…ç½®ä¿å­˜: {config_path}")
        
        # 3. åŠ è½½é…ç½®
        loaded_config = config_manager.load_configuration(config_path)
        assert loaded_config.base_model_name == self.config.base_model_name
        assert loaded_config.quantization_level == self.config.quantization_level
        print("   âœ“ é…ç½®åŠ è½½éªŒè¯é€šè¿‡")
        
        print("é…ç½®æ–‡ä»¶å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆï¼")
    
    @patch('src.format_exporter.ensure_disk_space')
    def test_error_handling_workflow(self, mock_ensure_disk):
        """æµ‹è¯•é”™è¯¯å¤„ç†å·¥ä½œæµç¨‹"""
        print("\n=== é”™è¯¯å¤„ç†å·¥ä½œæµç¨‹æµ‹è¯• ===")
        
        # 1. æµ‹è¯•ç£ç›˜ç©ºé—´ä¸è¶³é”™è¯¯
        from src.export_exceptions import DiskSpaceError
        mock_ensure_disk.side_effect = DiskSpaceError("ç£ç›˜ç©ºé—´ä¸è¶³")
        
        exporter = FormatExporter(self.config)
        
        with pytest.raises(Exception) as exc_info:
            exporter.export_pytorch_model(self.mock_model)
        
        assert "ç£ç›˜ç©ºé—´ä¸è¶³" in str(exc_info.value) or "PyTorchæ¨¡å‹å¯¼å‡ºå¤±è´¥" in str(exc_info.value)
        print("   âœ“ ç£ç›˜ç©ºé—´ä¸è¶³é”™è¯¯å¤„ç†æ­£ç¡®")
        
        # 2. éªŒè¯é”™è¯¯ç»Ÿè®¡
        export_stats = exporter.get_export_stats()
        assert export_stats['pytorch_export']['success'] is False
        assert export_stats['total_exports'] == 1
        assert export_stats['successful_exports'] == 0
        print("   âœ“ é”™è¯¯ç»Ÿè®¡è®°å½•æ­£ç¡®")
        
        print("é”™è¯¯å¤„ç†å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆï¼")
    
    def test_memory_optimization_workflow(self):
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–å·¥ä½œæµç¨‹"""
        print("\n=== å†…å­˜ä¼˜åŒ–å·¥ä½œæµç¨‹æµ‹è¯• ===")
        
        # åˆ›å»ºä¼˜åŒ–å¤„ç†å™¨
        optimizer = OptimizationProcessor(device="cpu", max_memory_gb=4.0)
        
        # 1. æµ‹è¯•ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
        print("1. æµ‹è¯•æƒé‡å‹ç¼©...")
        compressed_model = optimizer.compress_model_weights(self.mock_model)
        assert compressed_model is not None
        print("   âœ“ æƒé‡å‹ç¼©å®Œæˆ")
        
        print("2. æµ‹è¯•ç»“æ„ä¼˜åŒ–...")
        structure_optimized_model = optimizer.optimize_model_structure(compressed_model)
        assert structure_optimized_model is not None
        print("   âœ“ ç»“æ„ä¼˜åŒ–å®Œæˆ")
        
        print("3. æµ‹è¯•è®­ç»ƒartifactsæ¸…ç†...")
        cleaned_model = optimizer.remove_training_artifacts(structure_optimized_model)
        assert cleaned_model is not None
        print("   âœ“ è®­ç»ƒartifactsæ¸…ç†å®Œæˆ")
        
        # 2. è·å–ä¼˜åŒ–æŠ¥å‘Š
        optimization_report = optimizer.get_optimization_report()
        
        assert 'optimization_stats' in optimization_report
        assert 'system_info' in optimization_report
        
        optimization_steps = optimization_report['optimization_stats']['optimization_steps']
        assert len(optimization_steps) >= 3  # è‡³å°‘3ä¸ªä¼˜åŒ–æ­¥éª¤
        
        print(f"   âœ“ å®Œæˆ {len(optimization_steps)} ä¸ªä¼˜åŒ–æ­¥éª¤")
        
        # 3. æ¸…ç†å†…å­˜
        optimizer.cleanup()
        print("   âœ“ å†…å­˜æ¸…ç†å®Œæˆ")
        
        print("å†…å­˜ä¼˜åŒ–å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œé›†æˆæµ‹è¯•
    test_instance = TestPyTorchExportIntegration()
    test_instance.setup_method()
    
    try:
        test_instance.test_complete_pytorch_export_workflow()
        test_instance.test_configuration_file_workflow()
        test_instance.test_memory_optimization_workflow()
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"\nâŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        raise
    finally:
        test_instance.teardown_method()