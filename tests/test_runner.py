"""
æµ‹è¯•è¿è¡Œå™¨

è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶å¹¶ç”ŸæˆæŠ¥å‘Šã€‚
"""

import pytest
import sys
import os
from pathlib import Path
import time
import json
from datetime import datetime

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨ç±»"""
    
    def __init__(self, test_dir="tests", output_dir="test_results"):
        self.test_dir = Path(test_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def run_all_tests(self, verbose=True, coverage=True):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œè¯„ä¼°ç³»ç»Ÿæµ‹è¯•å¥—ä»¶...")
        
        start_time = time.time()
        
        # æ„å»ºpytestå‚æ•°
        pytest_args = [
            str(self.test_dir),
            "-v" if verbose else "-q",
            "--tb=short",
            f"--junitxml={self.output_dir}/test_results.xml",
            f"--html={self.output_dir}/test_report.html",
            "--self-contained-html"
        ]
        
        if coverage:
            pytest_args.extend([
                "--cov=evaluation",
                f"--cov-report=html:{self.output_dir}/coverage_html",
                f"--cov-report=xml:{self.output_dir}/coverage.xml",
                f"--cov-report=term-missing"
            ])
        
        # è¿è¡Œæµ‹è¯•
        exit_code = pytest.main(pytest_args)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ç”Ÿæˆæµ‹è¯•æ‘˜è¦
        self._generate_test_summary(exit_code, duration)
        
        return exit_code
    
    def run_unit_tests(self):
        """åªè¿è¡Œå•å…ƒæµ‹è¯•"""
        print("ğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
        
        unit_test_files = [
            "test_data_splitter.py",
            "test_evaluation_engine.py", 
            "test_metrics_calculator.py",
            "test_quality_analyzer.py",
            "test_benchmark_manager.py",
            "test_experiment_tracker.py",
            "test_report_generator.py",
            "test_efficiency_analyzer.py",
            "test_config_manager.py"
        ]
        
        pytest_args = [str(self.test_dir / test_file) for test_file in unit_test_files]
        pytest_args.extend(["-v", "--tb=short"])
        
        return pytest.main(pytest_args)
    
    def run_integration_tests(self):
        """åªè¿è¡Œé›†æˆæµ‹è¯•"""
        print("ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
        
        pytest_args = [
            str(self.test_dir / "test_integration_evaluation.py"),
            "-v",
            "--tb=short"
        ]
        
        return pytest.main(pytest_args)
    
    def run_performance_tests(self):
        """åªè¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        pytest_args = [
            str(self.test_dir / "test_performance_benchmarks.py"),
            "-v",
            "--tb=short",
            "-m", "performance"
        ]
        
        return pytest.main(pytest_args)
    
    def run_specific_test(self, test_file, test_function=None):
        """è¿è¡Œç‰¹å®šæµ‹è¯•"""
        test_path = str(self.test_dir / test_file)
        
        if test_function:
            test_path += f"::{test_function}"
        
        pytest_args = [test_path, "-v", "--tb=long"]
        
        return pytest.main(pytest_args)
    
    def run_failed_tests(self):
        """é‡æ–°è¿è¡Œå¤±è´¥çš„æµ‹è¯•"""
        print("ğŸ”„ é‡æ–°è¿è¡Œå¤±è´¥çš„æµ‹è¯•...")
        
        pytest_args = [
            str(self.test_dir),
            "--lf",  # last-failed
            "-v",
            "--tb=short"
        ]
        
        return pytest.main(pytest_args)
    
    def run_tests_with_markers(self, markers):
        """è¿è¡Œå¸¦ç‰¹å®šæ ‡è®°çš„æµ‹è¯•"""
        print(f"ğŸ·ï¸  è¿è¡Œæ ‡è®°ä¸º {markers} çš„æµ‹è¯•...")
        
        pytest_args = [
            str(self.test_dir),
            "-m", markers,
            "-v",
            "--tb=short"
        ]
        
        return pytest.main(pytest_args)
    
    def _generate_test_summary(self, exit_code, duration):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "duration": duration,
            "exit_code": exit_code,
            "status": "PASSED" if exit_code == 0 else "FAILED",
            "test_files": self._get_test_files(),
            "environment": {
                "python_version": sys.version,
                "platform": sys.platform,
                "working_directory": str(Path.cwd())
            }
        }
        
        # ä¿å­˜æ‘˜è¦
        summary_path = self.output_dir / "test_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print(f"{'='*60}")
        print(f"çŠ¶æ€: {'âœ… é€šè¿‡' if exit_code == 0 else 'âŒ å¤±è´¥'}")
        print(f"è€—æ—¶: {duration:.2f} ç§’")
        print(f"æµ‹è¯•æ–‡ä»¶æ•°: {len(summary['test_files'])}")
        print(f"æŠ¥å‘Šä½ç½®: {self.output_dir}")
        print(f"{'='*60}")
    
    def _get_test_files(self):
        """è·å–æ‰€æœ‰æµ‹è¯•æ–‡ä»¶"""
        return [f.name for f in self.test_dir.glob("test_*.py")]
    
    def generate_test_matrix(self):
        """ç”Ÿæˆæµ‹è¯•çŸ©é˜µ"""
        test_matrix = {
            "unit_tests": {
                "data_splitter": "test_data_splitter.py",
                "evaluation_engine": "test_evaluation_engine.py",
                "metrics_calculator": "test_metrics_calculator.py",
                "quality_analyzer": "test_quality_analyzer.py",
                "benchmark_manager": "test_benchmark_manager.py",
                "experiment_tracker": "test_experiment_tracker.py",
                "report_generator": "test_report_generator.py",
                "efficiency_analyzer": "test_efficiency_analyzer.py",
                "config_manager": "test_config_manager.py"
            },
            "integration_tests": {
                "evaluation_pipeline": "test_integration_evaluation.py"
            },
            "performance_tests": {
                "benchmarks": "test_performance_benchmarks.py"
            }
        }
        
        matrix_path = self.output_dir / "test_matrix.json"
        with open(matrix_path, 'w', encoding='utf-8') as f:
            json.dump(test_matrix, f, indent=2, ensure_ascii=False)
        
        return test_matrix
    
    def check_test_coverage(self):
        """æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡"""
        print("ğŸ“ˆ æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡...")
        
        # è¿è¡Œè¦†ç›–ç‡åˆ†æ
        pytest_args = [
            str(self.test_dir),
            "--cov=evaluation",
            "--cov-report=term-missing",
            "--cov-report=json",
            "--cov-fail-under=80",  # è¦æ±‚è‡³å°‘80%è¦†ç›–ç‡
            "-q"
        ]
        
        return pytest.main(pytest_args)
    
    def validate_test_environment(self):
        """éªŒè¯æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ” éªŒè¯æµ‹è¯•ç¯å¢ƒ...")
        
        required_packages = [
            "pytest",
            "pytest-cov",
            "pytest-html",
            "datasets",
            "numpy",
            "pandas"
        ]
        
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package.replace("-", "_"))
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print(f"âŒ ç¼ºå°‘å¿…éœ€çš„åŒ…: {', '.join(missing_packages)}")
            print("è¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
            return False
        
        print("âœ… æµ‹è¯•ç¯å¢ƒéªŒè¯é€šè¿‡")
        return True
    
    def clean_test_artifacts(self):
        """æ¸…ç†æµ‹è¯•äº§ç‰©"""
        print("ğŸ§¹ æ¸…ç†æµ‹è¯•äº§ç‰©...")
        
        # æ¸…ç†ç¼“å­˜ç›®å½•
        cache_dirs = [
            "__pycache__",
            ".pytest_cache",
            ".coverage"
        ]
        
        for cache_dir in cache_dirs:
            cache_path = Path(cache_dir)
            if cache_path.exists():
                import shutil
                if cache_path.is_dir():
                    shutil.rmtree(cache_path)
                else:
                    cache_path.unlink()
        
        # æ¸…ç†æµ‹è¯•è¾“å‡ºç›®å½•
        if self.output_dir.exists():
            import shutil
            shutil.rmtree(self.output_dir)
            self.output_dir.mkdir()
        
        print("âœ… æ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°ç³»ç»Ÿæµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--type", choices=["all", "unit", "integration", "performance"], 
                       default="all", help="æµ‹è¯•ç±»å‹")
    parser.add_argument("--coverage", action="store_true", default=True, help="ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š")
    parser.add_argument("--verbose", action="store_true", default=True, help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†æµ‹è¯•äº§ç‰©")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯æµ‹è¯•ç¯å¢ƒ")
    parser.add_argument("--file", help="è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶")
    parser.add_argument("--function", help="è¿è¡Œç‰¹å®šæµ‹è¯•å‡½æ•°")
    parser.add_argument("--markers", help="è¿è¡Œå¸¦ç‰¹å®šæ ‡è®°çš„æµ‹è¯•")
    parser.add_argument("--failed", action="store_true", help="é‡æ–°è¿è¡Œå¤±è´¥çš„æµ‹è¯•")
    
    args = parser.parse_args()
    
    runner = TestRunner()
    
    if args.clean:
        runner.clean_test_artifacts()
        return
    
    if args.validate:
        if not runner.validate_test_environment():
            sys.exit(1)
        return
    
    # ç”Ÿæˆæµ‹è¯•çŸ©é˜µ
    runner.generate_test_matrix()
    
    exit_code = 0
    
    if args.file:
        exit_code = runner.run_specific_test(args.file, args.function)
    elif args.failed:
        exit_code = runner.run_failed_tests()
    elif args.markers:
        exit_code = runner.run_tests_with_markers(args.markers)
    elif args.type == "unit":
        exit_code = runner.run_unit_tests()
    elif args.type == "integration":
        exit_code = runner.run_integration_tests()
    elif args.type == "performance":
        exit_code = runner.run_performance_tests()
    else:
        exit_code = runner.run_all_tests(args.verbose, args.coverage)
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()