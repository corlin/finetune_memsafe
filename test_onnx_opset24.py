#!/usr/bin/env python3
"""
æµ‹è¯•ONNX opsetç‰ˆæœ¬24

éªŒè¯æ–°çš„opsetç‰ˆæœ¬æ˜¯å¦èƒ½è§£å†³å¯¼å‡ºé—®é¢˜
"""

import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(".") / "src"))

from src.export_models import ExportConfiguration, LogLevel, QuantizationLevel

def test_onnx_opset24_config():
    """æµ‹è¯•ONNX opsetç‰ˆæœ¬24é…ç½®"""
    print("ğŸ§ª æµ‹è¯•ONNX opsetç‰ˆæœ¬24é…ç½®")
    print("=" * 40)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    print("1. åˆ›å»ºæµ‹è¯•é…ç½®...")
    config = ExportConfiguration(
        checkpoint_path="./qwen3-finetuned/checkpoint-30",
        base_model_name="Qwen/Qwen3-4B-Thinking-2507",
        output_directory="./test_output",
        quantization_level=QuantizationLevel.FP16,
        remove_training_artifacts=True,
        compress_weights=False,
        export_pytorch=True,
        export_onnx=True,
        export_tensorrt=False,
        onnx_opset_version=20,  # ä½¿ç”¨opsetç‰ˆæœ¬20
        onnx_optimize_graph=True,
        run_validation_tests=False,
        enable_progress_monitoring=True,
        log_level=LogLevel.INFO,
        max_memory_usage_gb=8.0
    )
    
    print(f"   âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"   ONNX opsetç‰ˆæœ¬: {config.onnx_opset_version}")
    print(f"   é‡åŒ–çº§åˆ«: {config.quantization_level.value}")
    print(f"   å¯¼å‡ºæ ¼å¼: PyTorch={config.export_pytorch}, ONNX={config.export_onnx}")
    
    # éªŒè¯é…ç½®
    print("2. éªŒè¯é…ç½®...")
    errors = config.validate()
    if errors:
        print(f"   âŒ é…ç½®éªŒè¯å¤±è´¥: {errors}")
        return False
    else:
        print("   âœ… é…ç½®éªŒè¯é€šè¿‡")
    
    print("\nğŸ‰ ONNX opsetç‰ˆæœ¬24é…ç½®æµ‹è¯•é€šè¿‡ï¼")
    print("é…ç½®å·²å‡†å¤‡å¥½ç”¨äºæ¨¡å‹å¯¼å‡º")
    
    return True

if __name__ == "__main__":
    try:
        success = test_onnx_opset24_config()
        sys.exit(0 if success else 1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)