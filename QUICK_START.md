# Qwen3-4B-Thinking-2507 å¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸš€ æ¨èå¯åŠ¨æ–¹å¼ï¼ˆè§£å†³uvæ„å»ºé—®é¢˜ï¼‰

ç”±äºuvçš„editableå®‰è£…å¯èƒ½é‡åˆ°æ„å»ºé—®é¢˜ï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹ç›´æ¥è¿è¡Œæ–¹å¼ï¼š

### æ–¹å¼1: ä½¿ç”¨Pythonç›´æ¥è¿è¡Œï¼ˆæ¨èï¼‰

```bash
# ç›´æ¥è¿è¡ŒPythonè„šæœ¬
python run_qwen3.py
```

æˆ–åœ¨Windowsä¸ŠåŒå‡» `run_qwen3.bat`

### æ–¹å¼2: æ‰‹åŠ¨å®‰è£…ä¾èµ–åè¿è¡Œ

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œä¸»ç¨‹åº
python main.py --model-name "Qwen/Qwen3-4B-Thinking-2507" --auto-install-deps
```

### æ–¹å¼3: ä½¿ç”¨uvä½†ç»•è¿‡editableå®‰è£…

```bash
# å®‰è£…ä¾èµ–åˆ°uvç¯å¢ƒ
uv pip install -r requirements.txt

# ç›´æ¥è¿è¡Œï¼ˆä¸ä½¿ç”¨é¡¹ç›®æ¨¡å¼ï¼‰
uv run --no-project python main.py --model-name "Qwen/Qwen3-4B-Thinking-2507" --auto-install-deps
```

## ğŸ”§ è¿è¡Œæ¨¡å¼é€‰æ‹©

è¿è¡Œ `python run_qwen3.py` åï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å¼ï¼š

1. **å¿«é€Ÿæµ‹è¯•æ¨¡å¼**: 5è½®è®­ç»ƒï¼ŒéªŒè¯ç¯å¢ƒæ˜¯å¦æ­£å¸¸
2. **ä½æ˜¾å­˜æ¨¡å¼**: é€‚åˆ6-8GB GPUï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
3. **æ ‡å‡†æ¨¡å¼**: æ¨èé…ç½®ï¼Œé€‚åˆ10GB+GPU
4. **é…ç½®æ–‡ä»¶æ¨¡å¼**: ä½¿ç”¨ `qwen3_4b_thinking_config.json` é…ç½®

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.12
- CUDAå…¼å®¹çš„GPU (æ¨è10GB+æ˜¾å­˜)
- 15GB+å¯ç”¨ç£ç›˜ç©ºé—´
- 16GB+ç³»ç»Ÿå†…å­˜

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¦‚æœé‡åˆ°ä¾èµ–å®‰è£…é—®é¢˜ï¼š

```bash
# æ–¹æ³•1: ä½¿ç”¨pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install transformers peft datasets bitsandbytes accelerate

# æ–¹æ³•2: ä½¿ç”¨uv
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
uv pip install transformers peft datasets bitsandbytes accelerate
```

### å¦‚æœé‡åˆ°GPUå†…å­˜ä¸è¶³ï¼š

é€‰æ‹©ä½æ˜¾å­˜æ¨¡å¼ï¼Œæˆ–æ‰‹åŠ¨è°ƒæ•´å‚æ•°ï¼š

```bash
python main.py \
  --model-name "Qwen/Qwen3-4B-Thinking-2507" \
  --max-memory-gb 6 \
  --batch-size 1 \
  --gradient-accumulation-steps 64 \
  --auto-install-deps
```

### å¦‚æœé‡åˆ°CUDAé—®é¢˜ï¼š

```bash
# æ£€æŸ¥CUDA
nvidia-smi

# æ£€æŸ¥PyTorch CUDAæ”¯æŒ
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“Š ç›‘æ§è®­ç»ƒ

è®­ç»ƒå¼€å§‹åï¼š

1. **æŸ¥çœ‹å®æ—¶æ—¥å¿—**:
   ```bash
   tail -f logs/application.log
   ```

2. **å¯åŠ¨TensorBoard**:
   ```bash
   tensorboard --logdir ./qwen3-finetuned/logs/tensorboard
   # è®¿é—® http://localhost:6006
   ```

3. **æ£€æŸ¥GPUä½¿ç”¨**:
   ```bash
   nvidia-smi
   ```

## ğŸ¯ å®Œæˆåçš„æ“ä½œ

è®­ç»ƒå®Œæˆåï¼Œæ‚¨ä¼šå¾—åˆ°ï¼š

- `qwen3-finetuned/` - å¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶
- `logs/` - è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—
- `final_application_report.json` - è®­ç»ƒæŠ¥å‘Š

æµ‹è¯•æ¨ç†ï¼š

```python
from src.inference_tester import InferenceTester

tester = InferenceTester()
tester.load_finetuned_model('./qwen3-finetuned', 'Qwen/Qwen3-4B-Thinking-2507')
response = tester.test_inference('è¯·è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ')
print(response)
```

## ğŸ’¡ æç¤º

- é¦–æ¬¡è¿è¡Œå»ºè®®é€‰æ‹©"å¿«é€Ÿæµ‹è¯•æ¨¡å¼"éªŒè¯ç¯å¢ƒ
- å¦‚æœGPUæ˜¾å­˜ä¸è¶³ï¼Œé€‰æ‹©"ä½æ˜¾å­˜æ¨¡å¼"
- è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥éšæ—¶æŒ‰Ctrl+Cä¸­æ–­ï¼ŒçŠ¶æ€ä¼šè‡ªåŠ¨ä¿å­˜
- å»ºè®®åœ¨è®­ç»ƒå‰å…³é—­å…¶ä»–å ç”¨GPUçš„ç¨‹åº