"""
æ¨¡å‹éªŒè¯æµ‹è¯•ç³»ç»Ÿ

æœ¬æ¨¡å—å®ç°äº†å®Œæ•´çš„æ¨¡å‹éªŒè¯æµ‹è¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬åŸºæœ¬åŠŸèƒ½æµ‹è¯•ã€å¤šæ ¼å¼æ¨¡å‹è¾“å‡ºå¯¹æ¯”ã€
æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œè¯¦ç»†çš„éªŒè¯æŠ¥å‘Šç”Ÿæˆã€‚
"""

import os
import time
import json
import torch
import numpy as np
import psutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import logging
import tempfile
import gc

from .export_models import ExportConfiguration, ValidationResult
from .export_exceptions import ValidationError, MemoryError
from .export_utils import format_size, ProgressTracker

# å°è¯•å¯¼å…¥ONNXç›¸å…³åº“
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

# å°è¯•å¯¼å…¥transformers
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ"""
    
    model_format: str
    model_path: str
    
    # æ¨ç†æ€§èƒ½
    avg_inference_time_ms: float = 0.0
    min_inference_time_ms: float = float('inf')
    max_inference_time_ms: float = 0.0
    throughput_tokens_per_sec: float = 0.0
    
    # å†…å­˜ä½¿ç”¨
    peak_memory_mb: float = 0.0
    avg_memory_mb: float = 0.0
    memory_efficiency: float = 0.0  # tokens per MB
    
    # æ¨¡å‹ä¿¡æ¯
    model_size_mb: float = 0.0
    parameter_count: int = 0
    
    # æµ‹è¯•é…ç½®
    test_samples: int = 0
    batch_size: int = 1
    sequence_length: int = 0
    
    # é”™è¯¯ä¿¡æ¯
    error_message: Optional[str] = None
    success: bool = True


@dataclass
class ValidationReport:
    """éªŒè¯æŠ¥å‘Š"""
    
    # åŸºæœ¬ä¿¡æ¯
    report_id: str
    timestamp: datetime
    config: ExportConfiguration
    
    # æµ‹è¯•ç»“æœ
    basic_tests: List[ValidationResult] = field(default_factory=list)
    comparison_tests: List[Dict[str, Any]] = field(default_factory=list)
    performance_benchmarks: List[PerformanceBenchmark] = field(default_factory=list)
    
    # æ€»ä½“ç»“æœ
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    success_rate: float = 0.0
    
    # æ€§èƒ½æ‘˜è¦
    best_performance: Optional[PerformanceBenchmark] = None
    performance_comparison: Dict[str, Any] = field(default_factory=dict)
    
    # å»ºè®®å’Œé—®é¢˜
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    def calculate_summary(self):
        """è®¡ç®—æŠ¥å‘Šæ‘˜è¦"""
        self.total_tests = len(self.basic_tests) + len(self.comparison_tests)
        self.passed_tests = sum(1 for test in self.basic_tests if test.success)
        self.passed_tests += sum(1 for test in self.comparison_tests if test.get('success', False))
        self.failed_tests = self.total_tests - self.passed_tests
        self.success_rate = self.passed_tests / self.total_tests if self.total_tests > 0 else 0.0
        
        # æ‰¾åˆ°æœ€ä½³æ€§èƒ½
        if self.performance_benchmarks:
            successful_benchmarks = [b for b in self.performance_benchmarks if b.success]
            if successful_benchmarks:
                self.best_performance = min(successful_benchmarks, 
                                          key=lambda x: x.avg_inference_time_ms)


class ValidationTester:
    """æ¨¡å‹éªŒè¯æµ‹è¯•å™¨"""
    
    def __init__(self, config: ExportConfiguration):
        """
        åˆå§‹åŒ–éªŒè¯æµ‹è¯•å™¨
        
        Args:
            config: å¯¼å‡ºé…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # æµ‹è¯•é…ç½®
        self.test_samples = [
            "Hello, how are you today?",
            "What is the capital of France?",
            "Explain quantum computing in simple terms.",
            "Write a short story about a robot.",
            "What are the benefits of renewable energy?",
            "How does machine learning work?",
            "Describe the process of photosynthesis.",
            "What is the meaning of life?",
            "How to cook a perfect pasta?",
            "Explain the theory of relativity."
        ]
        
        self.performance_test_samples = self.test_samples[:5]  # ä½¿ç”¨å‰5ä¸ªæ ·æœ¬è¿›è¡Œæ€§èƒ½æµ‹è¯•
        
        self.logger.info("éªŒè¯æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def run_comprehensive_validation(self, model_paths: Dict[str, str]) -> ValidationReport:
        """
        è¿è¡Œç»¼åˆéªŒè¯æµ‹è¯•
        
        Args:
            model_paths: æ¨¡å‹è·¯å¾„å­—å…¸ï¼Œæ ¼å¼ä¸º {'format': 'path'}
            
        Returns:
            ValidationReport: éªŒè¯æŠ¥å‘Š
        """
        report = ValidationReport(
            report_id=f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            timestamp=datetime.now(),
            config=self.config
        )
        
        try:
            self.logger.info("å¼€å§‹ç»¼åˆéªŒè¯æµ‹è¯•")
            
            # 1. åŸºæœ¬åŠŸèƒ½æµ‹è¯•
            self.logger.info("æ‰§è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•...")
            for format_name, model_path in model_paths.items():
                if os.path.exists(model_path):
                    basic_results = self.run_basic_functionality_tests(format_name, model_path)
                    report.basic_tests.extend(basic_results)
            
            # 2. å¤šæ ¼å¼è¾“å‡ºå¯¹æ¯”æµ‹è¯•
            if len(model_paths) > 1:
                self.logger.info("æ‰§è¡Œå¤šæ ¼å¼è¾“å‡ºå¯¹æ¯”æµ‹è¯•...")
                comparison_results = self.run_output_comparison_tests(model_paths)
                report.comparison_tests.extend(comparison_results)
            
            # 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
            self.logger.info("æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            for format_name, model_path in model_paths.items():
                if os.path.exists(model_path):
                    benchmark = self.run_performance_benchmark(format_name, model_path)
                    if benchmark:
                        report.performance_benchmarks.append(benchmark)
            
            # 4. è®¡ç®—æŠ¥å‘Šæ‘˜è¦
            report.calculate_summary()
            
            # 5. ç”Ÿæˆå»ºè®®å’Œé—®é¢˜
            self._generate_recommendations(report)
            
            self.logger.info(f"ç»¼åˆéªŒè¯æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {report.success_rate:.2%}")
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆéªŒè¯æµ‹è¯•å¤±è´¥: {str(e)}")
            report.issues.append(f"éªŒè¯æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        return report
    
    def run_basic_functionality_tests(self, format_name: str, model_path: str) -> List[ValidationResult]:
        """
        è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
        
        Args:
            format_name: æ¨¡å‹æ ¼å¼åç§°
            model_path: æ¨¡å‹è·¯å¾„
            
        Returns:
            List[ValidationResult]: æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            self.logger.info(f"å¼€å§‹{format_name}æ ¼å¼åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
            
            # æµ‹è¯•1: æ¨¡å‹åŠ è½½æµ‹è¯•
            load_result = self._test_model_loading(format_name, model_path)
            results.append(load_result)
            
            if load_result.success:
                # æµ‹è¯•2: åŸºæœ¬æ¨ç†æµ‹è¯•
                inference_result = self._test_basic_inference(format_name, model_path)
                results.append(inference_result)
                
                # æµ‹è¯•3: æ‰¹é‡æ¨ç†æµ‹è¯•
                batch_result = self._test_batch_inference(format_name, model_path)
                results.append(batch_result)
                
                # æµ‹è¯•4: è¾“å…¥éªŒè¯æµ‹è¯•
                input_validation_result = self._test_input_validation(format_name, model_path)
                results.append(input_validation_result)
            
        except Exception as e:
            error_result = ValidationResult(
                test_name=f"{format_name}_basic_functionality",
                success=False,
                error_message=f"åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}"
            )
            results.append(error_result)
        
        return results
    
    def run_output_comparison_tests(self, model_paths: Dict[str, str]) -> List[Dict[str, Any]]:
        """
        è¿è¡Œå¤šæ ¼å¼æ¨¡å‹è¾“å‡ºå¯¹æ¯”æµ‹è¯•
        
        Args:
            model_paths: æ¨¡å‹è·¯å¾„å­—å…¸
            
        Returns:
            List[Dict[str, Any]]: å¯¹æ¯”æµ‹è¯•ç»“æœ
        """
        results = []
        
        try:
            self.logger.info("å¼€å§‹å¤šæ ¼å¼è¾“å‡ºå¯¹æ¯”æµ‹è¯•")
            
            # è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æ ¼å¼
            available_formats = [fmt for fmt, path in model_paths.items() if os.path.exists(path)]
            
            if len(available_formats) < 2:
                self.logger.warning("å¯ç”¨æ¨¡å‹æ ¼å¼å°‘äº2ä¸ªï¼Œè·³è¿‡å¯¹æ¯”æµ‹è¯•")
                return results
            
            # å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œå¯¹æ¯”
            for i, test_input in enumerate(self.test_samples[:3]):  # ä½¿ç”¨å‰3ä¸ªæ ·æœ¬
                comparison_result = self._compare_model_outputs(
                    test_input, model_paths, available_formats, i
                )
                results.append(comparison_result)
            
        except Exception as e:
            self.logger.error(f"è¾“å‡ºå¯¹æ¯”æµ‹è¯•å¤±è´¥: {str(e)}")
            results.append({
                'test_name': 'output_comparison',
                'success': False,
                'error_message': str(e)
            })
        
        return results
    
    def run_performance_benchmark(self, format_name: str, model_path: str) -> Optional[PerformanceBenchmark]:
        """
        è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            format_name: æ¨¡å‹æ ¼å¼åç§°
            model_path: æ¨¡å‹è·¯å¾„
            
        Returns:
            Optional[PerformanceBenchmark]: æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
        """
        try:
            self.logger.info(f"å¼€å§‹{format_name}æ ¼å¼æ€§èƒ½åŸºå‡†æµ‹è¯•")
            
            benchmark = PerformanceBenchmark(
                model_format=format_name,
                model_path=model_path
            )
            
            # è·å–æ¨¡å‹å¤§å°
            if os.path.isfile(model_path):
                benchmark.model_size_mb = os.path.getsize(model_path) / (1024 * 1024)
            elif os.path.isdir(model_path):
                total_size = sum(
                    os.path.getsize(os.path.join(dirpath, filename))
                    for dirpath, dirnames, filenames in os.walk(model_path)
                    for filename in filenames
                )
                benchmark.model_size_mb = total_size / (1024 * 1024)
            
            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            if format_name.lower() == 'pytorch':
                self._benchmark_pytorch_model(model_path, benchmark)
            elif format_name.lower() == 'onnx' and ONNX_AVAILABLE:
                self._benchmark_onnx_model(model_path, benchmark)
            else:
                benchmark.success = False
                benchmark.error_message = f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format_name}"
            
            return benchmark
            
        except Exception as e:
            self.logger.error(f"{format_name}æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            return PerformanceBenchmark(
                model_format=format_name,
                model_path=model_path,
                success=False,
                error_message=str(e)
            )
    
    def generate_validation_report(self, report: ValidationReport, output_path: str):
        """
        ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
        
        Args:
            report: éªŒè¯æŠ¥å‘Šæ•°æ®
            output_path: è¾“å‡ºè·¯å¾„
        """
        try:
            self.logger.info(f"ç”ŸæˆéªŒè¯æŠ¥å‘Š: {output_path}")
            
            # åˆ›å»ºæŠ¥å‘Šç›®å½•
            report_dir = Path(output_path)
            report_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆJSONæŠ¥å‘Š
            json_report_path = report_dir / "validation_report.json"
            self._generate_json_report(report, json_report_path)
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_report_path = report_dir / "validation_report.html"
            self._generate_html_report(report, html_report_path)
            
            # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ ¼å¼ï¼‰
            if len(report.performance_benchmarks) > 1:
                chart_path = report_dir / "performance_comparison.json"
                self._generate_performance_chart_data(report, chart_path)
            
            self.logger.info("éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆéªŒè¯æŠ¥å‘Šå¤±è´¥: {str(e)}")
            raise ValidationError(f"ç”ŸæˆéªŒè¯æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def _test_model_loading(self, format_name: str, model_path: str) -> ValidationResult:
        """æµ‹è¯•æ¨¡å‹åŠ è½½"""
        start_time = time.time()
        
        try:
            if format_name.lower() == 'pytorch':
                if TRANSFORMERS_AVAILABLE:
                    # å°è¯•åŠ è½½PyTorchæ¨¡å‹
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map="cpu",
                        low_cpu_mem_usage=True
                    )
                    del model
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                else:
                    raise ImportError("transformersåº“ä¸å¯ç”¨")
                    
            elif format_name.lower() == 'onnx':
                if ONNX_AVAILABLE:
                    # å°è¯•åŠ è½½ONNXæ¨¡å‹
                    onnx_file = model_path if model_path.endswith('.onnx') else os.path.join(model_path, 'model.onnx')
                    session = ort.InferenceSession(onnx_file, providers=['CPUExecutionProvider'])
                    del session
                else:
                    raise ImportError("onnxruntimeåº“ä¸å¯ç”¨")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format_name}")
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name=f"{format_name}_model_loading",
                success=True,
                score=1.0,
                duration_seconds=duration,
                details={'load_time_seconds': duration}
            )
            
        except Exception as e:
            return ValidationResult(
                test_name=f"{format_name}_model_loading",
                success=False,
                error_message=str(e),
                duration_seconds=time.time() - start_time
            )
    
    def _test_basic_inference(self, format_name: str, model_path: str) -> ValidationResult:
        """æµ‹è¯•åŸºæœ¬æ¨ç†åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            test_input = "Hello, how are you?"
            
            if format_name.lower() == 'pytorch':
                result = self._pytorch_inference(model_path, test_input)
            elif format_name.lower() == 'onnx':
                result = self._onnx_inference(model_path, test_input)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format_name}")
            
            duration = time.time() - start_time
            
            # éªŒè¯è¾“å‡º
            if result and len(result) > 0:
                return ValidationResult(
                    test_name=f"{format_name}_basic_inference",
                    success=True,
                    score=1.0,
                    duration_seconds=duration,
                    details={
                        'input': test_input,
                        'output_length': len(result),
                        'inference_time_seconds': duration
                    }
                )
            else:
                return ValidationResult(
                    test_name=f"{format_name}_basic_inference",
                    success=False,
                    error_message="æ¨ç†è¾“å‡ºä¸ºç©º",
                    duration_seconds=duration
                )
                
        except Exception as e:
            return ValidationResult(
                test_name=f"{format_name}_basic_inference",
                success=False,
                error_message=str(e),
                duration_seconds=time.time() - start_time
            )
    
    def _test_batch_inference(self, format_name: str, model_path: str) -> ValidationResult:
        """æµ‹è¯•æ‰¹é‡æ¨ç†åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            test_inputs = self.test_samples[:3]  # ä½¿ç”¨å‰3ä¸ªæ ·æœ¬
            
            if format_name.lower() == 'pytorch':
                results = [self._pytorch_inference(model_path, inp) for inp in test_inputs]
            elif format_name.lower() == 'onnx':
                results = [self._onnx_inference(model_path, inp) for inp in test_inputs]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format_name}")
            
            duration = time.time() - start_time
            
            # éªŒè¯æ‰€æœ‰è¾“å‡º
            successful_results = [r for r in results if r and len(r) > 0]
            
            if len(successful_results) == len(test_inputs):
                return ValidationResult(
                    test_name=f"{format_name}_batch_inference",
                    success=True,
                    score=1.0,
                    duration_seconds=duration,
                    details={
                        'batch_size': len(test_inputs),
                        'successful_inferences': len(successful_results),
                        'avg_inference_time_seconds': duration / len(test_inputs)
                    }
                )
            else:
                return ValidationResult(
                    test_name=f"{format_name}_batch_inference",
                    success=False,
                    error_message=f"æ‰¹é‡æ¨ç†éƒ¨åˆ†å¤±è´¥: {len(successful_results)}/{len(test_inputs)}",
                    duration_seconds=duration
                )
                
        except Exception as e:
            return ValidationResult(
                test_name=f"{format_name}_batch_inference",
                success=False,
                error_message=str(e),
                duration_seconds=time.time() - start_time
            )
    
    def _test_input_validation(self, format_name: str, model_path: str) -> ValidationResult:
        """æµ‹è¯•è¾“å…¥éªŒè¯åŠŸèƒ½"""
        start_time = time.time()
        
        try:
            # æµ‹è¯•ä¸åŒç±»å‹çš„è¾“å…¥
            test_cases = [
                ("æ­£å¸¸è¾“å…¥", "This is a normal input."),
                ("ç©ºè¾“å…¥", ""),
                ("é•¿è¾“å…¥", "This is a very long input. " * 50),
                ("ç‰¹æ®Šå­—ç¬¦", "Hello! @#$%^&*()_+ ä½ å¥½ ğŸŒŸ"),
            ]
            
            results = []
            for case_name, test_input in test_cases:
                try:
                    if format_name.lower() == 'pytorch':
                        result = self._pytorch_inference(model_path, test_input)
                    elif format_name.lower() == 'onnx':
                        result = self._onnx_inference(model_path, test_input)
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {format_name}")
                    
                    results.append({
                        'case': case_name,
                        'input': test_input[:50] + "..." if len(test_input) > 50 else test_input,
                        'success': result is not None,
                        'output_length': len(result) if result else 0
                    })
                    
                except Exception as e:
                    results.append({
                        'case': case_name,
                        'input': test_input[:50] + "..." if len(test_input) > 50 else test_input,
                        'success': False,
                        'error': str(e)
                    })
            
            duration = time.time() - start_time
            successful_cases = sum(1 for r in results if r['success'])
            
            return ValidationResult(
                test_name=f"{format_name}_input_validation",
                success=successful_cases > 0,
                score=successful_cases / len(test_cases),
                duration_seconds=duration,
                details={
                    'test_cases': results,
                    'successful_cases': successful_cases,
                    'total_cases': len(test_cases)
                }
            )
            
        except Exception as e:
            return ValidationResult(
                test_name=f"{format_name}_input_validation",
                success=False,
                error_message=str(e),
                duration_seconds=time.time() - start_time
            )
    
    def _compare_model_outputs(self, test_input: str, model_paths: Dict[str, str], 
                              formats: List[str], test_index: int) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒæ ¼å¼æ¨¡å‹çš„è¾“å‡º"""
        try:
            outputs = {}
            
            # è·å–æ¯ä¸ªæ ¼å¼çš„è¾“å‡º
            for format_name in formats:
                model_path = model_paths[format_name]
                try:
                    if format_name.lower() == 'pytorch':
                        output = self._pytorch_inference(model_path, test_input)
                    elif format_name.lower() == 'onnx':
                        output = self._onnx_inference(model_path, test_input)
                    else:
                        continue
                    
                    outputs[format_name] = output
                    
                except Exception as e:
                    self.logger.warning(f"{format_name}æ ¼å¼æ¨ç†å¤±è´¥: {str(e)}")
                    outputs[format_name] = None
            
            # åˆ†æè¾“å‡ºä¸€è‡´æ€§
            valid_outputs = {k: v for k, v in outputs.items() if v is not None}
            
            if len(valid_outputs) < 2:
                return {
                    'test_name': f'output_comparison_{test_index}',
                    'test_input': test_input,
                    'success': False,
                    'error_message': 'å¯æ¯”è¾ƒçš„è¾“å‡ºå°‘äº2ä¸ª',
                    'outputs': outputs
                }
            
            # è®¡ç®—è¾“å‡ºç›¸ä¼¼æ€§ï¼ˆç®€å•çš„é•¿åº¦å’Œå†…å®¹æ¯”è¾ƒï¼‰
            output_lengths = {k: len(v) for k, v in valid_outputs.items()}
            output_similarities = {}
            
            format_list = list(valid_outputs.keys())
            for i in range(len(format_list)):
                for j in range(i + 1, len(format_list)):
                    fmt1, fmt2 = format_list[i], format_list[j]
                    similarity = self._calculate_text_similarity(
                        valid_outputs[fmt1], valid_outputs[fmt2]
                    )
                    output_similarities[f"{fmt1}_vs_{fmt2}"] = similarity
            
            avg_similarity = sum(output_similarities.values()) / len(output_similarities)
            
            return {
                'test_name': f'output_comparison_{test_index}',
                'test_input': test_input,
                'success': avg_similarity > 0.5,  # ç›¸ä¼¼åº¦é˜ˆå€¼
                'outputs': {k: v[:100] + "..." if v and len(v) > 100 else v 
                           for k, v in outputs.items()},
                'output_lengths': output_lengths,
                'similarities': output_similarities,
                'avg_similarity': avg_similarity,
                'details': {
                    'valid_formats': list(valid_outputs.keys()),
                    'comparison_count': len(output_similarities)
                }
            }
            
        except Exception as e:
            return {
                'test_name': f'output_comparison_{test_index}',
                'test_input': test_input,
                'success': False,
                'error_message': str(e)
            }
    
    def _benchmark_pytorch_model(self, model_path: str, benchmark: PerformanceBenchmark):
        """PyTorchæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if not TRANSFORMERS_AVAILABLE:
            benchmark.success = False
            benchmark.error_message = "transformersåº“ä¸å¯ç”¨"
            return
        
        try:
            # åŠ è½½æ¨¡å‹å’Œtokenizer
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
            except:
                # å¦‚æœæ²¡æœ‰tokenizerï¼Œä½¿ç”¨é»˜è®¤çš„
                tokenizer = AutoTokenizer.from_pretrained("gpt2")
                tokenizer.pad_token = tokenizer.eos_token
            
            model.eval()
            
            # è·å–å‚æ•°æ•°é‡
            benchmark.parameter_count = sum(p.numel() for p in model.parameters())
            
            # æ€§èƒ½æµ‹è¯•
            inference_times = []
            memory_usage = []
            
            process = psutil.Process()
            
            with torch.no_grad():
                for test_input in self.performance_test_samples:
                    # è®°å½•å†…å­˜ä½¿ç”¨
                    memory_before = process.memory_info().rss / (1024 * 1024)
                    
                    # æ¨ç†è®¡æ—¶
                    start_time = time.time()
                    
                    inputs = tokenizer(test_input, return_tensors="pt", padding=True, truncation=True)
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 20,
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    end_time = time.time()
                    
                    # è®°å½•å†…å­˜ä½¿ç”¨
                    memory_after = process.memory_info().rss / (1024 * 1024)
                    
                    inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    inference_times.append(inference_time)
                    memory_usage.append(memory_after)
                    
                    # è®¡ç®—åºåˆ—é•¿åº¦
                    if benchmark.sequence_length == 0:
                        benchmark.sequence_length = outputs.shape[1]
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            benchmark.avg_inference_time_ms = np.mean(inference_times)
            benchmark.min_inference_time_ms = np.min(inference_times)
            benchmark.max_inference_time_ms = np.max(inference_times)
            benchmark.peak_memory_mb = np.max(memory_usage)
            benchmark.avg_memory_mb = np.mean(memory_usage)
            benchmark.test_samples = len(self.performance_test_samples)
            
            # è®¡ç®—ååé‡
            if benchmark.avg_inference_time_ms > 0:
                benchmark.throughput_tokens_per_sec = (benchmark.sequence_length * 1000) / benchmark.avg_inference_time_ms
                benchmark.memory_efficiency = benchmark.throughput_tokens_per_sec / benchmark.avg_memory_mb if benchmark.avg_memory_mb > 0 else 0
            
            benchmark.success = True
            
            # æ¸…ç†å†…å­˜
            del model, tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            gc.collect()
            
        except Exception as e:
            benchmark.success = False
            benchmark.error_message = str(e)
    
    def _benchmark_onnx_model(self, model_path: str, benchmark: PerformanceBenchmark):
        """ONNXæ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        try:
            # ç¡®å®šONNXæ–‡ä»¶è·¯å¾„
            onnx_file = model_path if model_path.endswith('.onnx') else os.path.join(model_path, 'model.onnx')
            
            if not os.path.exists(onnx_file):
                benchmark.success = False
                benchmark.error_message = f"ONNXæ–‡ä»¶ä¸å­˜åœ¨: {onnx_file}"
                return
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            session = ort.InferenceSession(onnx_file, providers=['CPUExecutionProvider'])
            
            # å°è¯•åŠ è½½tokenizer
            try:
                tokenizer = AutoTokenizer.from_pretrained(os.path.dirname(onnx_file))
            except:
                tokenizer = AutoTokenizer.from_pretrained("gpt2")
                tokenizer.pad_token = tokenizer.eos_token
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            input_names = [inp.name for inp in session.get_inputs()]
            output_names = [out.name for out in session.get_outputs()]
            
            # æ€§èƒ½æµ‹è¯•
            inference_times = []
            memory_usage = []
            
            process = psutil.Process()
            
            for test_input in self.performance_test_samples:
                # è®°å½•å†…å­˜ä½¿ç”¨
                memory_before = process.memory_info().rss / (1024 * 1024)
                
                # å‡†å¤‡è¾“å…¥
                inputs = tokenizer(test_input, return_tensors="np", padding=True, truncation=True)
                onnx_inputs = {name: inputs[name] for name in input_names if name in inputs}
                
                # æ¨ç†è®¡æ—¶
                start_time = time.time()
                outputs = session.run(output_names, onnx_inputs)
                end_time = time.time()
                
                # è®°å½•å†…å­˜ä½¿ç”¨
                memory_after = process.memory_info().rss / (1024 * 1024)
                
                inference_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                inference_times.append(inference_time)
                memory_usage.append(memory_after)
                
                # è®¡ç®—åºåˆ—é•¿åº¦
                if benchmark.sequence_length == 0 and outputs:
                    benchmark.sequence_length = outputs[0].shape[1] if len(outputs[0].shape) > 1 else 1
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            benchmark.avg_inference_time_ms = np.mean(inference_times)
            benchmark.min_inference_time_ms = np.min(inference_times)
            benchmark.max_inference_time_ms = np.max(inference_times)
            benchmark.peak_memory_mb = np.max(memory_usage)
            benchmark.avg_memory_mb = np.mean(memory_usage)
            benchmark.test_samples = len(self.performance_test_samples)
            
            # è®¡ç®—ååé‡
            if benchmark.avg_inference_time_ms > 0:
                benchmark.throughput_tokens_per_sec = (benchmark.sequence_length * 1000) / benchmark.avg_inference_time_ms
                benchmark.memory_efficiency = benchmark.throughput_tokens_per_sec / benchmark.avg_memory_mb if benchmark.avg_memory_mb > 0 else 0
            
            benchmark.success = True
            
            # æ¸…ç†èµ„æº
            del session
            gc.collect()
            
        except Exception as e:
            benchmark.success = False
            benchmark.error_message = str(e)
    
    def _pytorch_inference(self, model_path: str, text: str) -> Optional[str]:
        """PyTorchæ¨¡å‹æ¨ç†"""
        if not TRANSFORMERS_AVAILABLE:
            return None
        
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
            except:
                tokenizer = AutoTokenizer.from_pretrained("gpt2")
                tokenizer.pad_token = tokenizer.eos_token
            
            model.eval()
            
            with torch.no_grad():
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                outputs = model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + 20,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æ¸…ç†å†…å­˜
                del model, tokenizer
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                return result
                
        except Exception as e:
            self.logger.warning(f"PyTorchæ¨ç†å¤±è´¥: {str(e)}")
            return None
    
    def _onnx_inference(self, model_path: str, text: str) -> Optional[str]:
        """ONNXæ¨¡å‹æ¨ç†"""
        if not ONNX_AVAILABLE:
            return None
        
        try:
            # ç¡®å®šONNXæ–‡ä»¶è·¯å¾„
            onnx_file = model_path if model_path.endswith('.onnx') else os.path.join(model_path, 'model.onnx')
            
            if not os.path.exists(onnx_file):
                return None
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            session = ort.InferenceSession(onnx_file, providers=['CPUExecutionProvider'])
            
            # åŠ è½½tokenizer
            try:
                tokenizer = AutoTokenizer.from_pretrained(os.path.dirname(onnx_file))
            except:
                tokenizer = AutoTokenizer.from_pretrained("gpt2")
                tokenizer.pad_token = tokenizer.eos_token
            
            # å‡†å¤‡è¾“å…¥
            inputs = tokenizer(text, return_tensors="np", padding=True, truncation=True)
            input_names = [inp.name for inp in session.get_inputs()]
            onnx_inputs = {name: inputs[name] for name in input_names if name in inputs}
            
            # æ‰§è¡Œæ¨ç†
            outputs = session.run(None, onnx_inputs)
            
            # è§£ç è¾“å‡ºï¼ˆç®€åŒ–å¤„ç†ï¼‰
            if outputs and len(outputs) > 0:
                logits = outputs[0]
                
                # å¤„ç†ä¸åŒå½¢çŠ¶çš„logits
                if isinstance(logits, (list, tuple)):
                    logits = np.array(logits)
                
                if len(logits.shape) > 2:
                    predicted_ids = np.argmax(logits[0], axis=-1)
                elif len(logits.shape) == 2:
                    predicted_ids = np.argmax(logits, axis=-1)
                else:
                    predicted_ids = logits
                
                result = tokenizer.decode(predicted_ids, skip_special_tokens=True)
                return result
            
            return None
            
        except Exception as e:
            self.logger.warning(f"ONNXæ¨ç†å¤±è´¥: {str(e)}")
            return None
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆç®€å•å®ç°ï¼‰"""
        if not text1 and not text2:
            return 1.0
        
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼æ€§è®¡ç®—
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _generate_recommendations(self, report: ValidationReport):
        """ç”Ÿæˆå»ºè®®å’Œé—®é¢˜"""
        # åˆ†ææµ‹è¯•ç»“æœ
        failed_tests = [test for test in report.basic_tests if not test.success]
        
        if failed_tests:
            report.issues.append(f"æœ‰{len(failed_tests)}ä¸ªåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            
            # åˆ†æå¤±è´¥åŸå› 
            error_types = {}
            for test in failed_tests:
                if test.error_message:
                    error_type = test.error_message.split(':')[0]
                    error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in error_types.items():
                report.issues.append(f"{error_type}é”™è¯¯å‡ºç°{count}æ¬¡")
        
        # æ€§èƒ½åˆ†æ
        if report.performance_benchmarks:
            successful_benchmarks = [b for b in report.performance_benchmarks if b.success]
            
            if successful_benchmarks:
                avg_inference_time = np.mean([b.avg_inference_time_ms for b in successful_benchmarks])
                if avg_inference_time > 1000:  # è¶…è¿‡1ç§’
                    report.issues.append("æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–")
                    report.recommendations.append("è€ƒè™‘ä½¿ç”¨é‡åŒ–æˆ–æ¨¡å‹å‹ç¼©æŠ€æœ¯")
                
                avg_memory = np.mean([b.avg_memory_mb for b in successful_benchmarks])
                if avg_memory > 1000:  # è¶…è¿‡1GB
                    report.issues.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜")
                    report.recommendations.append("è€ƒè™‘ä½¿ç”¨ä½ç²¾åº¦æ¨ç†æˆ–åˆ†æ‰¹å¤„ç†")
            else:
                report.issues.append("æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•éƒ½å¤±è´¥äº†")
                report.recommendations.append("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§å’Œä¾èµ–åº“å®‰è£…")
        
        # å¯¹æ¯”æµ‹è¯•åˆ†æ
        if report.comparison_tests:
            failed_comparisons = [test for test in report.comparison_tests if not test.get('success', False)]
            if failed_comparisons:
                report.issues.append(f"æœ‰{len(failed_comparisons)}ä¸ªè¾“å‡ºå¯¹æ¯”æµ‹è¯•å¤±è´¥")
                report.recommendations.append("æ£€æŸ¥ä¸åŒæ ¼å¼æ¨¡å‹çš„ä¸€è‡´æ€§")
        
        # é€šç”¨å»ºè®®
        if report.success_rate < 0.8:
            report.recommendations.append("æµ‹è¯•æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹å¯¼å‡ºè¿‡ç¨‹")
        
        if not report.issues:
            report.recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ¨¡å‹å¯¼å‡ºè´¨é‡è‰¯å¥½")
    
    def _generate_json_report(self, report: ValidationReport, output_path: Path):
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            'report_id': report.report_id,
            'timestamp': report.timestamp.isoformat(),
            'summary': {
                'total_tests': report.total_tests,
                'passed_tests': report.passed_tests,
                'failed_tests': report.failed_tests,
                'success_rate': report.success_rate
            },
            'basic_tests': [
                {
                    'test_name': test.test_name,
                    'success': test.success,
                    'score': test.score,
                    'duration_seconds': test.duration_seconds,
                    'error_message': test.error_message,
                    'details': test.details
                }
                for test in report.basic_tests
            ],
            'comparison_tests': report.comparison_tests,
            'performance_benchmarks': [
                {
                    'model_format': b.model_format,
                    'model_path': b.model_path,
                    'success': b.success,
                    'avg_inference_time_ms': b.avg_inference_time_ms,
                    'throughput_tokens_per_sec': b.throughput_tokens_per_sec,
                    'peak_memory_mb': b.peak_memory_mb,
                    'model_size_mb': b.model_size_mb,
                    'parameter_count': b.parameter_count,
                    'error_message': b.error_message
                }
                for b in report.performance_benchmarks
            ],
            'issues': report.issues,
            'recommendations': report.recommendations
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    def _generate_html_report(self, report: ValidationReport, output_path: Path):
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨¡å‹éªŒè¯æŠ¥å‘Š - {report.report_id}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ display: flex; justify-content: space-around; margin: 20px 0; }}
        .metric {{ text-align: center; }}
        .metric h3 {{ margin: 0; color: #333; }}
        .metric p {{ margin: 5px 0; font-size: 24px; font-weight: bold; }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
        .section {{ margin: 20px 0; }}
        .test-result {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ccc; }}
        .test-success {{ border-left-color: #28a745; }}
        .test-failure {{ border-left-color: #dc3545; }}
        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .issues {{ background-color: #fff3cd; padding: 15px; border-radius: 5px; }}
        .recommendations {{ background-color: #d1ecf1; padding: 15px; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>æ¨¡å‹éªŒè¯æŠ¥å‘Š</h1>
        <p><strong>æŠ¥å‘ŠID:</strong> {report.report_id}</p>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>æ€»æµ‹è¯•æ•°</h3>
            <p>{report.total_tests}</p>
        </div>
        <div class="metric">
            <h3>é€šè¿‡æµ‹è¯•</h3>
            <p class="success">{report.passed_tests}</p>
        </div>
        <div class="metric">
            <h3>å¤±è´¥æµ‹è¯•</h3>
            <p class="failure">{report.failed_tests}</p>
        </div>
        <div class="metric">
            <h3>æˆåŠŸç‡</h3>
            <p class="{'success' if report.success_rate >= 0.8 else 'failure'}">{report.success_rate:.1%}</p>
        </div>
    </div>
    
    <div class="section">
        <h2>åŸºæœ¬åŠŸèƒ½æµ‹è¯•</h2>
        {''.join([f'''
        <div class="test-result {'test-success' if test.success else 'test-failure'}">
            <h4>{test.test_name}</h4>
            <p><strong>çŠ¶æ€:</strong> {'é€šè¿‡' if test.success else 'å¤±è´¥'}</p>
            <p><strong>è€—æ—¶:</strong> {test.duration_seconds:.2f}ç§’</p>
            {f'<p><strong>é”™è¯¯:</strong> {test.error_message}</p>' if test.error_message else ''}
        </div>
        ''' for test in report.basic_tests])}
    </div>
    
    {f'''
    <div class="section">
        <h2>æ€§èƒ½åŸºå‡†æµ‹è¯•</h2>
        <table>
            <tr>
                <th>æ¨¡å‹æ ¼å¼</th>
                <th>å¹³å‡æ¨ç†æ—¶é—´(ms)</th>
                <th>ååé‡(tokens/s)</th>
                <th>å³°å€¼å†…å­˜(MB)</th>
                <th>æ¨¡å‹å¤§å°(MB)</th>
                <th>çŠ¶æ€</th>
            </tr>
            {''.join([f'''
            <tr>
                <td>{b.model_format}</td>
                <td>{b.avg_inference_time_ms:.2f}</td>
                <td>{b.throughput_tokens_per_sec:.2f}</td>
                <td>{b.peak_memory_mb:.2f}</td>
                <td>{b.model_size_mb:.2f}</td>
                <td class="{'success' if b.success else 'failure'}">{'æˆåŠŸ' if b.success else 'å¤±è´¥'}</td>
            </tr>
            ''' for b in report.performance_benchmarks])}
        </table>
    </div>
    ''' if report.performance_benchmarks else ''}
    
    {f'''
    <div class="section issues">
        <h2>å‘ç°çš„é—®é¢˜</h2>
        <ul>
            {''.join([f'<li>{issue}</li>' for issue in report.issues])}
        </ul>
    </div>
    ''' if report.issues else ''}
    
    {f'''
    <div class="section recommendations">
        <h2>å»ºè®®</h2>
        <ul>
            {''.join([f'<li>{rec}</li>' for rec in report.recommendations])}
        </ul>
    </div>
    ''' if report.recommendations else ''}
    
</body>
</html>
        """
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _generate_performance_chart_data(self, report: ValidationReport, output_path: Path):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨æ•°æ®"""
        chart_data = {
            'labels': [b.model_format for b in report.performance_benchmarks if b.success],
            'datasets': [
                {
                    'label': 'å¹³å‡æ¨ç†æ—¶é—´(ms)',
                    'data': [b.avg_inference_time_ms for b in report.performance_benchmarks if b.success],
                    'backgroundColor': 'rgba(54, 162, 235, 0.2)',
                    'borderColor': 'rgba(54, 162, 235, 1)'
                },
                {
                    'label': 'å³°å€¼å†…å­˜ä½¿ç”¨(MB)',
                    'data': [b.peak_memory_mb for b in report.performance_benchmarks if b.success],
                    'backgroundColor': 'rgba(255, 99, 132, 0.2)',
                    'borderColor': 'rgba(255, 99, 132, 1)'
                },
                {
                    'label': 'ååé‡(tokens/s)',
                    'data': [b.throughput_tokens_per_sec for b in report.performance_benchmarks if b.success],
                    'backgroundColor': 'rgba(75, 192, 192, 0.2)',
                    'borderColor': 'rgba(75, 192, 192, 1)'
                }
            ]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chart_data, f, indent=2)