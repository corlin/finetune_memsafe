#!/usr/bin/env python3
"""
PyTorchæ¨¡å‹å¯¼å‡ºåŠŸèƒ½æ¼”ç¤º

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ¨¡å‹å¯¼å‡ºä¼˜åŒ–ç³»ç»Ÿæ¥å¯¼å‡ºPyTorchæ ¼å¼çš„æ¨¡å‹ã€‚
"""

import os
import tempfile
import shutil
from pathlib import Path
import torch
import torch.nn as nn

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from src.export_config import ConfigurationManager
from src.export_models import ExportConfiguration, QuantizationLevel
from src.optimization_processor import OptimizationProcessor
from src.format_exporter import FormatExporter


class DemoModel(nn.Module):
    """æ¼”ç¤ºç”¨çš„ç®€å•æ¨¡å‹"""
    
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(1000, 256)
        self.transformer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)
        self.classifier = nn.Linear(256, 10)
        
        # æ¨¡æ‹Ÿé…ç½®
        class Config:
            def __init__(self):
                self.model_type = 'demo_model'
                self.vocab_size = 1000
                self.hidden_size = 256
                self.num_layers = 1
            
            def to_dict(self):
                return {
                    'model_type': 'demo_model',
                    'vocab_size': 1000,
                    'hidden_size': 256,
                    'num_layers': 1,
                    'architectures': ['DemoModel']
                }
        
        self.config = Config()
    
    def forward(self, input_ids):
        x = self.embedding(input_ids)
        x = self.transformer(x)
        x = x.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ–
        return self.classifier(x)
    
    def save_pretrained(self, path, **kwargs):
        """æ¨¡æ‹Ÿtransformersçš„save_pretrainedæ–¹æ³•"""
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        import json
        config_path = path / "config.json"
        with open(config_path, 'w') as f:
            json.dump(self.config.to_dict(), f, indent=2)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(self.state_dict(), path / "pytorch_model.bin")
        
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„safetensorsæ–‡ä»¶
        (path / "model.safetensors").write_bytes(b"mock_safetensors_data" * 1000)


class DemoTokenizer:
    """æ¼”ç¤ºç”¨çš„ç®€å•tokenizer"""
    
    def __init__(self):
        self.vocab_size = 1000
        self.pad_token = "<pad>"
        self.eos_token = "<eos>"
    
    def save_pretrained(self, path):
        """æ¨¡æ‹Ÿä¿å­˜tokenizer"""
        path = Path(path)
        
        # åˆ›å»ºtokenizeré…ç½®
        import json
        tokenizer_config = {
            "tokenizer_class": "DemoTokenizer",
            "vocab_size": self.vocab_size,
            "pad_token": self.pad_token,
            "eos_token": self.eos_token
        }
        
        with open(path / "tokenizer_config.json", 'w') as f:
            json.dump(tokenizer_config, f, indent=2)
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„tokenizer.json
        with open(path / "tokenizer.json", 'w') as f:
            json.dump({"version": "1.0", "vocab": {}}, f)


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ PyTorchæ¨¡å‹å¯¼å‡ºåŠŸèƒ½æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    checkpoint_dir = os.path.join(temp_dir, "demo_checkpoint")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    try:
        # 1. åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œtokenizer
        print("\nğŸ“¦ 1. åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œtokenizer")
        model = DemoModel()
        tokenizer = DemoTokenizer()
        
        print(f"   âœ“ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   âœ“ æ¨¡å‹å¤§å°: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024:.1f} MB")
        
        # 2. é…ç½®å¯¼å‡ºå‚æ•°
        print("\nâš™ï¸  2. é…ç½®å¯¼å‡ºå‚æ•°")
        ##checkpoint_dir,
        #"demo/demo-model",
        config = ExportConfiguration(
            checkpoint_path="qwen3-finetuned/checkpoint-30",
            base_model_name="Qwen/Qwen3-4B-Thinking-2507",
            output_directory=temp_dir,
            quantization_level=QuantizationLevel.INT8,
            export_pytorch=True,
            save_tokenizer=True,
            run_validation_tests=False
        )
        
        print(f"   âœ“ è¾“å‡ºç›®å½•: {config.output_directory}")
        print(f"   âœ“ é‡åŒ–çº§åˆ«: {config.quantization_level.value}")
        print(f"   âœ“ å¯¼å‡ºæ ¼å¼: PyTorch")
        
        # 3. æ¨¡å‹ä¼˜åŒ–
        print("\nğŸ”§ 3. æ¨¡å‹ä¼˜åŒ–å¤„ç†")
        optimizer = OptimizationProcessor(device="cpu", max_memory_gb=4.0)
        
        # åº”ç”¨ä¼˜åŒ–
        print("   æ­£åœ¨ç§»é™¤è®­ç»ƒartifacts...")
        optimized_model = optimizer.remove_training_artifacts(model)
        
        print("   æ­£åœ¨å‹ç¼©æ¨¡å‹æƒé‡...")
        optimized_model = optimizer.compress_model_weights(optimized_model)
        
        # è·å–ä¼˜åŒ–ç»Ÿè®¡
        optimization_report = optimizer.get_optimization_report()
        optimization_steps = optimization_report['optimization_stats']['optimization_steps']
        
        print(f"   âœ“ å®Œæˆ {len(optimization_steps)} ä¸ªä¼˜åŒ–æ­¥éª¤")
        for i, step in enumerate(optimization_steps, 1):
            print(f"     {i}. {step['step']}: {step['size_reduction_percentage']:.1f}% å¤§å°å‡å°‘")
        
        # 4. PyTorchæ ¼å¼å¯¼å‡º
        print("\nğŸ“¤ 4. PyTorchæ ¼å¼å¯¼å‡º")
        exporter = FormatExporter(config)
        
        print("   æ­£åœ¨å¯¼å‡ºPyTorchæ¨¡å‹...")
        export_path = exporter.export_pytorch_model(optimized_model, tokenizer)
        
        print(f"   âœ“ å¯¼å‡ºå®Œæˆ: {export_path}")
        
        # æ£€æŸ¥å¯¼å‡ºçš„æ–‡ä»¶
        export_path_obj = Path(export_path)
        exported_files = list(export_path_obj.iterdir())
        print(f"   âœ“ å¯¼å‡ºæ–‡ä»¶æ•°é‡: {len(exported_files)}")
        
        for file_path in exported_files:
            size_mb = file_path.stat().st_size / 1024 / 1024
            print(f"     - {file_path.name}: {size_mb:.2f} MB")
        
        # 5. éªŒè¯å¯¼å‡ºç»“æœ
        print("\nâœ… 5. éªŒè¯å¯¼å‡ºç»“æœ")
        
        # æ£€æŸ¥å¯¼å‡ºç»Ÿè®¡
        export_stats = exporter.get_export_stats()
        print(f"   âœ“ å¯¼å‡ºæˆåŠŸ: {export_stats['pytorch_export']['success']}")
        print(f"   âœ“ å¯¼å‡ºå¤§å°: {export_stats['pytorch_export']['size_mb']:.1f} MB")
        print(f"   âœ“ æ€»å¯¼å‡ºæ¬¡æ•°: {export_stats['total_exports']}")
        print(f"   âœ“ æˆåŠŸå¯¼å‡ºæ¬¡æ•°: {export_stats['successful_exports']}")
        
        # æ£€æŸ¥å…ƒæ•°æ®
        metadata_path = export_path_obj / 'export_metadata.json'
        if metadata_path.exists():
            import json
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            print(f"   âœ“ å¯¼å‡ºæ—¶é—´: {metadata['export_info']['export_timestamp']}")
            print(f"   âœ“ æ¨¡å‹ç±»å‹: {metadata['model_info']['model_type']}")
            print(f"   âœ“ å‚æ•°æ•°é‡: {metadata['model_info']['parameter_count']:,}")
        
        # 6. ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹
        print("\nğŸ“ 6. ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹")
        usage_example = exporter.generate_usage_example(export_path)
        
        example_path = export_path_obj / "usage_example.py"
        if example_path.exists():
            print(f"   âœ“ ä½¿ç”¨ç¤ºä¾‹å·²ä¿å­˜: {example_path}")
            print("   âœ“ ç¤ºä¾‹å†…å®¹é¢„è§ˆ:")
            with open(example_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()[:10]  # æ˜¾ç¤ºå‰10è¡Œ
                for i, line in enumerate(lines, 1):
                    print(f"     {i:2d}: {line.rstrip()}")
                if len(lines) == 10:
                    print("     ... (æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹æ–‡ä»¶)")
        
        # 7. åˆ›å»ºéƒ¨ç½²åŒ…
        print("\nğŸ“¦ 7. åˆ›å»ºéƒ¨ç½²åŒ…")
        try:
            package_path = exporter.create_deployment_package(export_path)
            package_size = Path(package_path).stat().st_size / 1024 / 1024
            print(f"   âœ“ éƒ¨ç½²åŒ…åˆ›å»ºæˆåŠŸ: {Path(package_path).name}")
            print(f"   âœ“ éƒ¨ç½²åŒ…å¤§å°: {package_size:.1f} MB")
        except Exception as e:
            print(f"   âš ï¸  éƒ¨ç½²åŒ…åˆ›å»ºè·³è¿‡: {e}")
        
        # 8. æ€»ç»“
        print("\nğŸ‰ å¯¼å‡ºå®Œæˆæ€»ç»“")
        print("=" * 30)
        print(f"å¯¼å‡ºè·¯å¾„: {export_path}")
        print(f"å¯¼å‡ºå¤§å°: {export_stats['pytorch_export']['size_mb']:.1f} MB")
        print(f"ä¼˜åŒ–æ­¥éª¤: {len(optimization_steps)}")
        print(f"å¯¼å‡ºæ–‡ä»¶: {len(exported_files)}")
        
        print("\nâœ¨ PyTorchæ¨¡å‹å¯¼å‡ºæ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {temp_dir}")
        
        # è¯¢é—®æ˜¯å¦ä¿ç•™æ–‡ä»¶
        try:
            keep_files = input("\næ˜¯å¦ä¿ç•™å¯¼å‡ºçš„æ–‡ä»¶ï¼Ÿ(y/N): ").lower().strip()
            if keep_files in ['y', 'yes']:
                print(f"æ–‡ä»¶å·²ä¿ç•™åœ¨: {temp_dir}")
                return temp_dir
        except (KeyboardInterrupt, EOFError):
            pass
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœç”¨æˆ·é€‰æ‹©ä¸ä¿ç•™ï¼‰
        try:
            if 'keep_files' not in locals() or keep_files not in ['y', 'yes']:
                shutil.rmtree(temp_dir)
                print(f"\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    main()