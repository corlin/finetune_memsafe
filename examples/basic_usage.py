#!/usr/bin/env python3
"""
åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºæ•°æ®æ‹†åˆ†å’Œè¯„ä¼°ç³»ç»Ÿçš„åŸºæœ¬ç”¨æ³•ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from datasets import Dataset
from evaluation import (
    DataSplitter, EvaluationEngine, EvaluationConfig,
    ExperimentTracker, ReportGenerator
)
from evaluation.data_models import ExperimentConfig


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    data = [
        {"text": "è¿™æ˜¯ä¸€ä¸ªæ­£é¢çš„è¯„è®º", "label": "positive"},
        {"text": "è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨", "label": "positive"},
        {"text": "è´¨é‡ä¸é”™ï¼Œæ¨èè´­ä¹°", "label": "positive"},
        {"text": "è¿™ä¸ªäº§å“ä¸å¤ªå¥½", "label": "negative"},
        {"text": "è´¨é‡å¾ˆå·®ï¼Œä¸æ¨è", "label": "negative"},
        {"text": "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼", "label": "negative"},
        {"text": "è¿˜å¯ä»¥ï¼Œä¸€èˆ¬èˆ¬", "label": "neutral"},
        {"text": "æ²¡ä»€ä¹ˆç‰¹åˆ«çš„", "label": "neutral"},
        {"text": "æ™®é€šçš„äº§å“", "label": "neutral"},
        {"text": "ä»·æ ¼åˆç†ï¼Œè´¨é‡è¿˜è¡Œ", "label": "neutral"},
    ] * 20  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
    
    return Dataset.from_list(data)


def mock_model_and_tokenizer():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹å’Œåˆ†è¯å™¨"""
    class MockModel:
        def __init__(self):
            self.config = type('Config', (), {'is_encoder_decoder': False})()
        
        def generate(self, **kwargs):
            # æ¨¡æ‹Ÿç”Ÿæˆç»“æœ
            import torch
            batch_size = kwargs['input_ids'].shape[0]
            seq_len = kwargs.get('max_length', 50)
            return torch.randint(0, 1000, (batch_size, seq_len))
        
        def __call__(self, **kwargs):
            # æ¨¡æ‹Ÿåˆ†ç±»è¾“å‡º
            import torch
            batch_size = kwargs['input_ids'].shape[0]
            return type('Output', (), {
                'logits': torch.randn(batch_size, 3)  # 3ä¸ªç±»åˆ«
            })()
    
    class MockTokenizer:
        def __init__(self):
            self.pad_token_id = 0
        
        def __call__(self, texts, **kwargs):
            import torch
            if isinstance(texts, str):
                texts = [texts]
            
            batch_size = len(texts)
            seq_len = kwargs.get('max_length', 50)
            
            return {
                'input_ids': torch.randint(0, 1000, (batch_size, seq_len)),
                'attention_mask': torch.ones(batch_size, seq_len)
            }
        
        def decode(self, token_ids, **kwargs):
            return f"decoded_text_{len(token_ids)}"
        
        def batch_decode(self, batch_token_ids, **kwargs):
            return [f"decoded_text_{i}" for i in range(len(batch_token_ids))]
    
    return MockModel(), MockTokenizer()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹...")
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("\nğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    dataset = create_sample_data()
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {dataset['label'][:10]}")
    
    # 2. æ•°æ®æ‹†åˆ†
    print("\nâœ‚ï¸ æ‰§è¡Œæ•°æ®æ‹†åˆ†...")
    splitter = DataSplitter(
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
        random_seed=42,
        stratify_by="label",
        min_samples_per_split=10
    )
    
    split_result = splitter.split_data(dataset, "output/basic_example_splits")
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(split_result.train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(split_result.val_dataset)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(split_result.test_dataset)}")
    print(f"æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§åˆ†æ•°: {split_result.distribution_analysis.consistency_score}")
    
    # 3. åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œåˆ†è¯å™¨...")
    model, tokenizer = mock_model_and_tokenizer()
    
    # 4. é…ç½®è¯„ä¼°
    print("\nâš™ï¸ é…ç½®è¯„ä¼°å‚æ•°...")
    config = EvaluationConfig(
        tasks=["classification"],
        metrics=["accuracy", "precision", "recall", "f1"],
        batch_size=4,
        num_samples=50  # ä½¿ç”¨è¾ƒå°‘æ ·æœ¬ä»¥åŠ å¿«æ¼”ç¤º
    )
    
    # 5. è¿è¡Œè¯„ä¼°
    print("\nğŸ§ª è¿è¡Œæ¨¡å‹è¯„ä¼°...")
    engine = EvaluationEngine(config)
    
    # å‡†å¤‡è¯„ä¼°æ•°æ®é›†
    datasets = {
        "classification": split_result.test_dataset
    }
    
    # æ¨¡æ‹Ÿæ¨ç†å‡½æ•°
    def mock_inference_function(inputs):
        """æ¨¡æ‹Ÿæ¨ç†å‡½æ•°"""
        import random
        labels = ["positive", "negative", "neutral"]
        return [random.choice(labels) for _ in inputs]
    
    # æ›¿æ¢æ¨ç†å‡½æ•°ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ä¸éœ€è¦è¿™æ­¥ï¼‰
    engine._create_inference_function = lambda model, tokenizer: mock_inference_function
    
    result = engine.evaluate_model(
        model=model,
        tokenizer=tokenizer,
        datasets=datasets,
        model_name="basic_example_model"
    )
    
    print("è¯„ä¼°ç»“æœ:")
    for metric, value in result.metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    # 6. å®éªŒè·Ÿè¸ª
    print("\nğŸ“ è·Ÿè¸ªå®éªŒ...")
    tracker = ExperimentTracker(experiment_dir="output/basic_example_experiments")
    
    experiment_config = ExperimentConfig(
        experiment_name="basic_usage_example",
        model_config={
            "model_name": "basic_example_model",
            "model_type": "mock_model"
        },
        training_config={
            "batch_size": config.batch_size,
            "num_samples": config.num_samples
        },
        evaluation_config=config,
        data_config={
            "dataset_name": "sample_sentiment_data",
            "data_size": len(dataset)
        },
        tags=["example", "basic_usage"],
        description="åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹å®éªŒ"
    )
    
    experiment_id = tracker.track_experiment(experiment_config, result)
    print(f"å®éªŒID: {experiment_id}")
    
    # 7. ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    generator = ReportGenerator(
        output_dir="output/basic_example_reports"
    )
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_report = generator.generate_evaluation_report(result, format_type="html")
    print(f"HTMLæŠ¥å‘Š: {html_report}")
    
    # ç”ŸæˆJSONæŠ¥å‘Š
    json_report = generator.generate_evaluation_report(result, format_type="json")
    print(f"JSONæŠ¥å‘Š: {json_report}")
    
    # 8. æŸ¥çœ‹å®éªŒå†å²
    print("\nğŸ“‹ æŸ¥çœ‹å®éªŒå†å²...")
    experiments = tracker.list_experiments()
    print(f"æ€»å®éªŒæ•°: {len(experiments)}")
    
    if experiments:
        latest_exp = experiments[0]
        # è·å–å®Œæ•´çš„å®éªŒä¿¡æ¯ä»¥è®¿é—®metadata
        full_exp = tracker.get_experiment(latest_exp['id'])
        if full_exp and 'metadata' in full_exp:
            model_name = full_exp['metadata'].get('model_config', {}).get('model_name', 'æœªçŸ¥æ¨¡å‹')
        else:
            model_name = 'æœªçŸ¥æ¨¡å‹'
        print(f"æœ€æ–°å®éªŒ: {model_name}")
        print(f"å®éªŒæ—¶é—´: {latest_exp['created_at']}")
    
    # 9. å¯¼å‡ºç»“æœ
    print("\nğŸ’¾ å¯¼å‡ºå®éªŒç»“æœ...")
    csv_path = tracker.export_results("output/basic_example_results.csv", format="csv")
    print(f"CSVå¯¼å‡º: {csv_path}")
    
    print("\nâœ… åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print("  - æ•°æ®æ‹†åˆ†: output/basic_example_splits/")
    print("  - å®éªŒè®°å½•: output/basic_example_experiments/")
    print("  - è¯„ä¼°æŠ¥å‘Š: output/basic_example_reports/")
    print("  - ç»“æœå¯¼å‡º: output/basic_example_results.csv")


if __name__ == "__main__":
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    
    try:
        main()
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
