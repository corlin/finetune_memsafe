"""
Industry Evaluation System å®Œæ•´åŠŸèƒ½æ¼”ç¤º

è¿™ä¸ªç¤ºä¾‹ç¨‹åºå±•ç¤ºäº†è¡Œä¸šè¯„ä¼°ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é…ç½®ç®¡ç†
2. æ¨¡å‹é€‚é…å™¨
3. è¯„ä¼°å™¨
4. è¯„ä¼°å¼•æ“
5. æ‰¹é‡è¯„ä¼°
6. æŠ¥å‘Šç”Ÿæˆ
7. APIæ¥å£
"""

import asyncio
import json
import logging
import tempfile
import time
import sys
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
from industry_evaluation.config.config_manager import (
    ConfigManager, ConfigTemplate, ModelConfig, EvaluatorConfig
)
from industry_evaluation.adapters.model_adapter import (
    ModelManager, ModelAdapterFactory
)
from industry_evaluation.core.evaluation_engine import IndustryEvaluationEngine
from industry_evaluation.core.batch_evaluator import BatchEvaluator, BatchEvaluationConfig
from industry_evaluation.core.result_aggregator import ResultAggregator
from industry_evaluation.reporting.report_generator import ReportGenerator
from industry_evaluation.evaluators.knowledge_evaluator import KnowledgeEvaluator
from industry_evaluation.evaluators.terminology_evaluator import TerminologyEvaluator
from industry_evaluation.evaluators.reasoning_evaluator import ReasoningEvaluator
from industry_evaluation.evaluators.long_text_evaluator import LongTextEvaluator
from industry_evaluation.core.interfaces import EvaluationConfig
from industry_evaluation.api.rest_api import EvaluationAPI


class MockModelAdapter:
    """æ¼”ç¤ºç”¨çš„æ¨¡æ‹Ÿæ¨¡å‹é€‚é…å™¨"""
    
    def __init__(self, model_id: str, config: Dict[str, Any]):
        self.model_id = model_id
        self.config = config
        self.quality = config.get("quality", "good")
        self.domain = config.get("domain", "general")
        
        # æ¨¡æ‹Ÿä¸åŒè´¨é‡çš„çŸ¥è¯†åº“
        self.knowledge_base = {
            "finance": {
                "VaR": "Value at Riskæ˜¯ä¸€ç§é£é™©åº¦é‡æ–¹æ³•ï¼Œç”¨äºé‡åŒ–åœ¨æ­£å¸¸å¸‚åœºæ¡ä»¶ä¸‹ï¼Œç‰¹å®šæ—¶é—´æ®µå†…æŠ•èµ„ç»„åˆå¯èƒ½é¢ä¸´çš„æœ€å¤§æŸå¤±ã€‚",
                "è¡ç”Ÿå“": "è¡ç”Ÿå“æ˜¯ä¸€ç§é‡‘èå·¥å…·ï¼Œå…¶ä»·å€¼æ¥æºäºåŸºç¡€èµ„äº§çš„ä»·æ ¼å˜åŠ¨ï¼ŒåŒ…æ‹¬æœŸè´§ã€æœŸæƒã€æ‰æœŸç­‰ã€‚",
                "æµåŠ¨æ€§é£é™©": "æµåŠ¨æ€§é£é™©æ˜¯æŒ‡æ— æ³•åœ¨åˆç†æ—¶é—´å†…ä»¥åˆç†ä»·æ ¼å˜ç°èµ„äº§çš„é£é™©ã€‚"
            },
            "healthcare": {
                "è¯Šæ–­": "åŒ»å­¦è¯Šæ–­æ˜¯é€šè¿‡ç—‡çŠ¶ã€ä½“å¾ã€å®éªŒå®¤æ£€æŸ¥ç­‰ä¿¡æ¯ç¡®å®šç–¾ç—…çš„è¿‡ç¨‹ã€‚",
                "æ²»ç–—æ–¹æ¡ˆ": "æ²»ç–—æ–¹æ¡ˆæ˜¯é’ˆå¯¹ç‰¹å®šç–¾ç—…åˆ¶å®šçš„ç»¼åˆæ²»ç–—è®¡åˆ’ï¼ŒåŒ…æ‹¬è¯ç‰©æ²»ç–—ã€æ‰‹æœ¯æ²»ç–—ç­‰ã€‚",
                "é¢„å": "é¢„åæ˜¯å¯¹ç–¾ç—…å‘å±•è¶‹åŠ¿å’Œæ²»ç–—æ•ˆæœçš„é¢„æµ‹ã€‚"
            },
            "technology": {
                "åŒºå—é“¾": "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦æ–¹æ³•ç¡®ä¿æ•°æ®çš„ä¸å¯ç¯¡æ”¹æ€§ã€‚",
                "äººå·¥æ™ºèƒ½": "äººå·¥æ™ºèƒ½æ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ã€‚",
                "äº‘è®¡ç®—": "äº‘è®¡ç®—æ˜¯é€šè¿‡ç½‘ç»œæä¾›å¯æ‰©å±•çš„è®¡ç®—èµ„æºå’ŒæœåŠ¡çš„æ¨¡å¼ã€‚"
            }
        }
    
    def predict(self, input_text: str, context=None):
        """æ ¹æ®è¾“å…¥ç”Ÿæˆç›¸åº”çš„å›ç­”"""
        domain = context.get("industry", "general") if context else "general"
        
        # æ ¹æ®è´¨é‡å’Œé¢†åŸŸç”Ÿæˆä¸åŒè´¨é‡çš„å›ç­”
        if self.quality == "excellent" and domain in self.knowledge_base:
            # é«˜è´¨é‡å›ç­”ï¼šå‡†ç¡®ä¸”è¯¦ç»†
            for keyword, definition in self.knowledge_base[domain].items():
                if keyword in input_text:
                    return f"{definition} è¿™æ˜¯ä¸€ä¸ªåœ¨{domain}é¢†åŸŸä¸­éå¸¸é‡è¦çš„æ¦‚å¿µï¼Œéœ€è¦æ·±å…¥ç†è§£å…¶åº”ç”¨åœºæ™¯å’Œå½±å“å› ç´ ã€‚"
            
            return f"é’ˆå¯¹{domain}é¢†åŸŸçš„é—®é¢˜'{input_text}'ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸“ä¸šé—®é¢˜ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ è¿›è¡Œåˆ†æã€‚"
        
        elif self.quality == "good":
            # ä¸­ç­‰è´¨é‡å›ç­”ï¼šåŸºæœ¬å‡†ç¡®
            for keyword, definition in self.knowledge_base.get(domain, {}).items():
                if keyword in input_text:
                    return f"{definition}"
            
            return f"å…³äº'{input_text}'çš„é—®é¢˜ï¼Œè¿™æ¶‰åŠåˆ°{domain}é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚"
        
        else:
            # ä½è´¨é‡å›ç­”ï¼šç®€å•æˆ–å¯èƒ½æœ‰é”™è¯¯
            return f"å…³äº'{input_text}'ï¼Œè¿™æ˜¯ä¸€ä¸ª{domain}ç›¸å…³çš„é—®é¢˜ã€‚"
    
    def is_available(self):
        return True


class IndustryEvaluationDemo:
    """è¡Œä¸šè¯„ä¼°ç³»ç»Ÿå®Œæ•´æ¼”ç¤º"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ"""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.config_file = self.temp_dir / "demo_config.yaml"
        
        logger.info(f"æ¼”ç¤ºç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œä¸´æ—¶ç›®å½•: {self.temp_dir}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.setup_configuration()
        self.setup_models()
        self.setup_evaluators()
        self.setup_evaluation_engine()
        self.setup_test_data()
    
    def setup_configuration(self):
        """è®¾ç½®é…ç½®ç®¡ç†"""
        logger.info("ğŸ”§ è®¾ç½®é…ç½®ç®¡ç†ç³»ç»Ÿ...")
        
        # åˆ›å»ºé‡‘èè¡Œä¸šé…ç½®æ¨¡æ¿
        config = ConfigTemplate.generate_finance_config()
        
        # æ·»åŠ æ¼”ç¤ºæ¨¡å‹é…ç½®
        config.models = {
            "finance_expert": ModelConfig(
                model_id="finance_expert",
                adapter_type="demo",
                timeout=30,
                max_retries=3,
                retry_config={
                    "strategy": "exponential_backoff",
                    "base_delay": 1.0,
                    "max_delay": 10.0
                }
            ),
            "general_model": ModelConfig(
                model_id="general_model",
                adapter_type="demo",
                timeout=30,
                max_retries=2
            ),
            "poor_model": ModelConfig(
                model_id="poor_model",
                adapter_type="demo",
                timeout=30,
                max_retries=1
            )
        }
        
        # ä¿å­˜é…ç½®
        ConfigTemplate.save_template(config, self.config_file)
        
        # åˆ›å»ºé…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(self.config_file, auto_reload=False)
        
        logger.info("âœ… é…ç½®ç®¡ç†ç³»ç»Ÿè®¾ç½®å®Œæˆ")
    
    def setup_models(self):
        """è®¾ç½®æ¨¡å‹ç®¡ç†"""
        logger.info("ğŸ¤– è®¾ç½®æ¨¡å‹ç®¡ç†ç³»ç»Ÿ...")
        
        # æ³¨å†Œæ¼”ç¤ºæ¨¡å‹é€‚é…å™¨
        ModelAdapterFactory.register_adapter("demo", MockModelAdapter)
        
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager()
        
        # æ³¨å†Œä¸åŒè´¨é‡çš„æ¨¡å‹
        self.model_manager.register_model(
            "finance_expert", 
            "demo", 
            {"quality": "excellent", "domain": "finance"}
        )
        
        self.model_manager.register_model(
            "general_model", 
            "demo", 
            {"quality": "good", "domain": "general"}
        )
        
        self.model_manager.register_model(
            "poor_model", 
            "demo", 
            {"quality": "poor", "domain": "general"}
        )
        
        logger.info("âœ… æ¨¡å‹ç®¡ç†ç³»ç»Ÿè®¾ç½®å®Œæˆ")
    
    def setup_evaluators(self):
        """è®¾ç½®è¯„ä¼°å™¨"""
        logger.info("ğŸ“Š è®¾ç½®è¯„ä¼°å™¨ç³»ç»Ÿ...")
        
        self.evaluators = {
            "knowledge": KnowledgeEvaluator(),
            "terminology": TerminologyEvaluator(),
            "reasoning": ReasoningEvaluator(),
            "long_text": LongTextEvaluator()
        }
        
        logger.info("âœ… è¯„ä¼°å™¨ç³»ç»Ÿè®¾ç½®å®Œæˆ")
    
    def setup_evaluation_engine(self):
        """è®¾ç½®è¯„ä¼°å¼•æ“"""
        logger.info("ğŸš€ è®¾ç½®è¯„ä¼°å¼•æ“...")
        
        # åˆ›å»ºæ ¸å¿ƒç»„ä»¶
        self.result_aggregator = ResultAggregator()
        self.report_generator = ReportGenerator()
        
        # åˆ›å»ºè¯„ä¼°å¼•æ“
        self.evaluation_engine = IndustryEvaluationEngine(
            model_manager=self.model_manager,
            evaluators=self.evaluators,
            result_aggregator=self.result_aggregator,
            report_generator=self.report_generator,
            max_workers=2
        )
        
        # åˆ›å»ºæ‰¹é‡è¯„ä¼°å™¨
        self.batch_evaluator = BatchEvaluator(self.evaluation_engine)
        
        logger.info("âœ… è¯„ä¼°å¼•æ“è®¾ç½®å®Œæˆ")
    
    def setup_test_data(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        logger.info("ğŸ“ å‡†å¤‡æµ‹è¯•æ•°æ®...")
        
        # é‡‘èé¢†åŸŸæµ‹è¯•æ•°æ®
        self.finance_dataset = [
            {
                "id": "finance_1",
                "input": "è¯·è§£é‡Šé‡‘èé£é™©ç®¡ç†ä¸­çš„VaRæ¨¡å‹åŠå…¶åº”ç”¨",
                "expected_output": "VaRï¼ˆValue at Riskï¼‰æ˜¯ä¸€ç§é£é™©åº¦é‡æ–¹æ³•ï¼Œç”¨äºé‡åŒ–åœ¨æ­£å¸¸å¸‚åœºæ¡ä»¶ä¸‹ï¼Œç‰¹å®šæ—¶é—´æ®µå†…æŠ•èµ„ç»„åˆå¯èƒ½é¢ä¸´çš„æœ€å¤§æŸå¤±ã€‚å®ƒé€šå¸¸ç”¨95%æˆ–99%çš„ç½®ä¿¡æ°´å¹³æ¥è®¡ç®—ï¼Œå¹¿æ³›åº”ç”¨äºé“¶è¡Œã€æŠ•èµ„å…¬å¸ç­‰é‡‘èæœºæ„çš„é£é™©ç®¡ç†ä¸­ã€‚",
                "context": {
                    "industry": "finance",
                    "topic": "risk_management",
                    "difficulty": "intermediate"
                }
            },
            {
                "id": "finance_2",
                "input": "ä»€ä¹ˆæ˜¯é‡‘èè¡ç”Ÿå“ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜å…¶ä¸»è¦ç±»å‹",
                "expected_output": "é‡‘èè¡ç”Ÿå“æ˜¯ä¸€ç§é‡‘èå·¥å…·ï¼Œå…¶ä»·å€¼æ¥æºäºåŸºç¡€èµ„äº§çš„ä»·æ ¼å˜åŠ¨ã€‚ä¸»è¦ç±»å‹åŒ…æ‹¬ï¼š1ï¼‰æœŸè´§åˆçº¦ï¼šæ ‡å‡†åŒ–çš„è¿œæœŸåˆçº¦ï¼›2ï¼‰æœŸæƒï¼šç»™äºˆæŒæœ‰è€…åœ¨ç‰¹å®šæ—¶é—´ä»¥ç‰¹å®šä»·æ ¼ä¹°å–èµ„äº§çš„æƒåˆ©ï¼›3ï¼‰æ‰æœŸï¼šäº¤æ¢ç°é‡‘æµçš„åè®®ï¼›4ï¼‰è¿œæœŸåˆçº¦ï¼šéæ ‡å‡†åŒ–çš„æœªæ¥äº¤æ˜“åè®®ã€‚",
                "context": {
                    "industry": "finance",
                    "topic": "derivatives",
                    "difficulty": "basic"
                }
            },
            {
                "id": "finance_3",
                "input": "å¦‚ä½•è¯„ä¼°å’Œç®¡ç†é“¶è¡Œçš„æµåŠ¨æ€§é£é™©ï¼Ÿ",
                "expected_output": "é“¶è¡ŒæµåŠ¨æ€§é£é™©ç®¡ç†åŒ…æ‹¬ï¼š1ï¼‰æµåŠ¨æ€§è¦†ç›–ç‡ï¼ˆLCRï¼‰ç›‘æ§ï¼›2ï¼‰å‡€ç¨³å®šèµ„é‡‘æ¯”ç‡ï¼ˆNSFRï¼‰ç®¡ç†ï¼›3ï¼‰å‹åŠ›æµ‹è¯•ï¼›4ï¼‰å¤šå…ƒåŒ–èµ„é‡‘æ¥æºï¼›5ï¼‰å»ºç«‹æµåŠ¨æ€§ç¼“å†²ï¼›6ï¼‰åˆ¶å®šåº”æ€¥æµåŠ¨æ€§è®¡åˆ’ã€‚å…³é”®æ˜¯å¹³è¡¡æµåŠ¨æ€§éœ€æ±‚ä¸ç›ˆåˆ©æ€§ã€‚",
                "context": {
                    "industry": "finance",
                    "topic": "liquidity_risk",
                    "difficulty": "advanced"
                }
            }
        ]
        
        # åˆ›å»ºå¤§æ•°æ®é›†ç”¨äºæ‰¹é‡æµ‹è¯•
        self.large_dataset = []
        topics = ["é£é™©ç®¡ç†", "æŠ•èµ„ç­–ç•¥", "å¸‚åœºåˆ†æ", "é‡‘èåˆ›æ–°", "ç›‘ç®¡åˆè§„"]
        
        for i in range(50):
            topic = topics[i % len(topics)]
            self.large_dataset.append({
                "id": f"large_sample_{i}",
                "input": f"è¯·åˆ†æ{topic}ç›¸å…³çš„é—®é¢˜ {i}ï¼šå¦‚ä½•åœ¨å½“å‰å¸‚åœºç¯å¢ƒä¸‹ä¼˜åŒ–ç­–ç•¥ï¼Ÿ",
                "expected_output": f"å…³äº{topic}çš„ä¸“ä¸šåˆ†æ {i}ï¼šéœ€è¦ç»¼åˆè€ƒè™‘å¸‚åœºç¯å¢ƒã€é£é™©å› ç´ å’Œç›‘ç®¡è¦æ±‚ã€‚",
                "context": {
                    "industry": "finance",
                    "topic": topic.replace(" ", "_"),
                    "sample_index": i
                }
            })
        
        # ä¿å­˜å¤§æ•°æ®é›†åˆ°æ–‡ä»¶
        self.large_dataset_path = self.temp_dir / "large_dataset.json"
        with open(self.large_dataset_path, 'w', encoding='utf-8') as f:
            json.dump(self.large_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ")
    
    async def demo_single_evaluation(self):
        """æ¼”ç¤ºå•æ¨¡å‹è¯„ä¼°"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ æ¼”ç¤º1: å•æ¨¡å‹è¯„ä¼°")
        logger.info("="*60)
        
        # é…ç½®è¯„ä¼°å‚æ•°
        config = EvaluationConfig(
            industry_domain="finance",
            evaluation_dimensions=["knowledge", "terminology", "reasoning"],
            weight_config={
                "knowledge": 0.5,
                "terminology": 0.3,
                "reasoning": 0.2
            },
            threshold_config={
                "knowledge": 0.7,
                "terminology": 0.6,
                "reasoning": 0.7
            },
            auto_generate_report=True
        )
        
        logger.info("ğŸ“‹ è¯„ä¼°é…ç½®:")
        logger.info(f"  - è¡Œä¸šé¢†åŸŸ: {config.industry_domain}")
        logger.info(f"  - è¯„ä¼°ç»´åº¦: {config.evaluation_dimensions}")
        logger.info(f"  - æƒé‡é…ç½®: {config.weight_config}")
        
        # å¯åŠ¨è¯„ä¼°
        logger.info("ğŸš€ å¯åŠ¨é‡‘èä¸“å®¶æ¨¡å‹è¯„ä¼°...")
        task_id = self.evaluation_engine.evaluate_model(
            model_id="finance_expert",
            dataset=self.finance_dataset,
            evaluation_config=config
        )
        
        logger.info(f"ğŸ“ è¯„ä¼°ä»»åŠ¡ID: {task_id}")
        
        # ç›‘æ§è¯„ä¼°è¿›åº¦
        await self._monitor_evaluation_progress(task_id)
        
        # è·å–è¯„ä¼°ç»“æœ
        result = self.evaluation_engine.get_evaluation_result(task_id)
        
        if result:
            logger.info("ğŸ“Š è¯„ä¼°ç»“æœ:")
            logger.info(f"  - ç»¼åˆå¾—åˆ†: {result.overall_score:.3f}")
            logger.info(f"  - çŸ¥è¯†å¾—åˆ†: {result.dimension_scores.get('knowledge', 0):.3f}")
            logger.info(f"  - æœ¯è¯­å¾—åˆ†: {result.dimension_scores.get('terminology', 0):.3f}")
            logger.info(f"  - æ¨ç†å¾—åˆ†: {result.dimension_scores.get('reasoning', 0):.3f}")
            logger.info(f"  - å¤„ç†æ ·æœ¬æ•°: {len(result.detailed_results)}")
            
            if result.improvement_suggestions:
                logger.info("ğŸ’¡ æ”¹è¿›å»ºè®®:")
                for suggestion in result.improvement_suggestions[:3]:
                    logger.info(f"  - {suggestion}")
        
        return task_id
    
    async def demo_model_comparison(self):
        """æ¼”ç¤ºæ¨¡å‹å¯¹æ¯”è¯„ä¼°"""
        logger.info("\n" + "="*60)
        logger.info("âš–ï¸ æ¼”ç¤º2: æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        logger.info("="*60)
        
        config = EvaluationConfig(
            industry_domain="finance",
            evaluation_dimensions=["knowledge", "terminology"],
            weight_config={"knowledge": 0.7, "terminology": 0.3},
            threshold_config={"knowledge": 0.6, "terminology": 0.5}
        )
        
        models_to_compare = ["finance_expert", "general_model", "poor_model"]
        results = {}
        
        logger.info(f"ğŸ”„ å¼€å§‹å¯¹æ¯” {len(models_to_compare)} ä¸ªæ¨¡å‹...")
        
        # å¹¶è¡Œè¯„ä¼°å¤šä¸ªæ¨¡å‹
        tasks = []
        for model_id in models_to_compare:
            logger.info(f"ğŸš€ å¯åŠ¨æ¨¡å‹ {model_id} çš„è¯„ä¼°...")
            task_id = self.evaluation_engine.evaluate_model(
                model_id=model_id,
                dataset=self.finance_dataset[:2],  # ä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†
                evaluation_config=config
            )
            tasks.append((model_id, task_id))
        
        # ç­‰å¾…æ‰€æœ‰è¯„ä¼°å®Œæˆ
        for model_id, task_id in tasks:
            await self._monitor_evaluation_progress(task_id, model_name=model_id)
            result = self.evaluation_engine.get_evaluation_result(task_id)
            if result:
                results[model_id] = result
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        logger.info("\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        logger.info("-" * 80)
        logger.info(f"{'æ¨¡å‹åç§°':<15} {'ç»¼åˆå¾—åˆ†':<10} {'çŸ¥è¯†å¾—åˆ†':<10} {'æœ¯è¯­å¾—åˆ†':<10}")
        logger.info("-" * 80)
        
        for model_id, result in results.items():
            logger.info(
                f"{model_id:<15} "
                f"{result.overall_score:<10.3f} "
                f"{result.dimension_scores.get('knowledge', 0):<10.3f} "
                f"{result.dimension_scores.get('terminology', 0):<10.3f}"
            )
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        if results:
            best_model = max(results.items(), key=lambda x: x[1].overall_score)
            logger.info(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (å¾—åˆ†: {best_model[1].overall_score:.3f})")
        
        return results
    
    async def demo_batch_evaluation(self):
        """æ¼”ç¤ºæ‰¹é‡è¯„ä¼°"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“¦ æ¼”ç¤º3: æ‰¹é‡è¯„ä¼°")
        logger.info("="*60)
        
        # é…ç½®æ‰¹é‡è¯„ä¼°
        eval_config = EvaluationConfig(
            industry_domain="finance",
            evaluation_dimensions=["knowledge", "terminology"],
            weight_config={"knowledge": 0.8, "terminology": 0.2},
            threshold_config={"knowledge": 0.6, "terminology": 0.5}
        )
        
        batch_config = BatchEvaluationConfig(
            batch_size=10,
            max_concurrent_tasks=2,
            chunk_size=20,
            save_intermediate_results=True,
            intermediate_results_dir=str(self.temp_dir / "batch_results"),
            enable_parallel_processing=False  # ä½¿ç”¨é¡ºåºå¤„ç†ä»¥ä¾¿è§‚å¯Ÿ
        )
        
        logger.info("ğŸ“‹ æ‰¹é‡è¯„ä¼°é…ç½®:")
        logger.info(f"  - æ•°æ®é›†å¤§å°: {len(self.large_dataset)} æ ·æœ¬")
        logger.info(f"  - æ‰¹æ¬¡å¤§å°: {batch_config.batch_size}")
        logger.info(f"  - è¯„ä¼°æ¨¡å‹: ['finance_expert', 'general_model']")
        
        # åˆ›å»ºæ‰¹é‡ä»»åŠ¡
        batch_task = self.batch_evaluator.create_batch_task(
            task_id="demo_batch_evaluation",
            model_ids=["finance_expert", "general_model"],
            dataset_path=str(self.large_dataset_path),
            evaluation_config=eval_config,
            batch_config=batch_config
        )
        
        logger.info(f"ğŸ“ æ‰¹é‡ä»»åŠ¡ID: {batch_task.task_id}")
        logger.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {batch_task.total_samples}")
        
        # å¯åŠ¨æ‰¹é‡è¯„ä¼°
        success = self.batch_evaluator.start_batch_evaluation("demo_batch_evaluation")
        
        if success:
            logger.info("ğŸš€ æ‰¹é‡è¯„ä¼°å·²å¯åŠ¨...")
            
            # ç›‘æ§æ‰¹é‡è¯„ä¼°è¿›åº¦
            await self._monitor_batch_evaluation_progress("demo_batch_evaluation")
            
            # è·å–æœ€ç»ˆç»“æœ
            final_task = self.batch_evaluator.get_batch_task_status("demo_batch_evaluation")
            
            if final_task and final_task.status == "completed":
                logger.info("\nğŸ“Š æ‰¹é‡è¯„ä¼°ç»“æœ:")
                logger.info(f"  - çŠ¶æ€: {final_task.status}")
                logger.info(f"  - å¤„ç†æ ·æœ¬: {final_task.processed_samples}/{final_task.total_samples}")
                logger.info(f"  - å¤±è´¥æ ·æœ¬: {final_task.failed_samples}")
                
                if final_task.results:
                    logger.info("  - æ¨¡å‹ç»“æœ:")
                    for model_id, result in final_task.results.items():
                        logger.info(f"    * {model_id}: {result.overall_score:.3f} (æ ·æœ¬æ•°: {len(result.detailed_results)})")
        
        return batch_task
    
    async def demo_report_generation(self, task_id: str):
        """æ¼”ç¤ºæŠ¥å‘Šç”Ÿæˆ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“„ æ¼”ç¤º4: æŠ¥å‘Šç”Ÿæˆ")
        logger.info("="*60)
        
        logger.info("ğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š
        json_report = self.evaluation_engine.generate_report(task_id, "json")
        
        if json_report:
            logger.info("âœ… JSONæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            
            # è§£æå¹¶æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
            if isinstance(json_report, str):
                try:
                    report_data = json.loads(json_report)
                    logger.info("ğŸ“Š æŠ¥å‘Šæ‘˜è¦:")
                    logger.info(f"  - ç»¼åˆå¾—åˆ†: {report_data.get('overall_score', 'N/A')}")
                    logger.info(f"  - è¯„ä¼°ç»´åº¦: {list(report_data.get('dimension_scores', {}).keys())}")
                    logger.info(f"  - è¡Œä¸šé¢†åŸŸ: {report_data.get('industry_domain', 'N/A')}")
                    
                    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
                    report_file = self.temp_dir / f"evaluation_report_{task_id}.json"
                    with open(report_file, 'w', encoding='utf-8') as f:
                        json.dump(report_data, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
                    
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ æŠ¥å‘Šæ ¼å¼è§£æå¤±è´¥")
        else:
            logger.warning("âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
    
    async def demo_api_interface(self):
        """æ¼”ç¤ºAPIæ¥å£"""
        logger.info("\n" + "="*60)
        logger.info("ğŸŒ æ¼”ç¤º5: APIæ¥å£")
        logger.info("="*60)
        
        try:
            # åˆ›å»ºAPIå®ä¾‹
            api = EvaluationAPI(self.config_manager)
            app = api.get_app()
            
            # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
            with app.test_client() as client:
                logger.info("ğŸ” æµ‹è¯•APIç«¯ç‚¹...")
                
                # æµ‹è¯•å¥åº·æ£€æŸ¥
                response = client.get('/health')
                logger.info(f"  - å¥åº·æ£€æŸ¥: {response.status_code} - {json.loads(response.data)['status']}")
                
                # æµ‹è¯•ç³»ç»Ÿä¿¡æ¯
                response = client.get('/info')
                if response.status_code == 200:
                    info = json.loads(response.data)
                    logger.info(f"  - ç³»ç»Ÿä¿¡æ¯: ç‰ˆæœ¬ {info.get('version', 'N/A')}")
                
                # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
                response = client.get('/models')
                if response.status_code == 200:
                    models = json.loads(response.data)
                    logger.info(f"  - æ¨¡å‹åˆ—è¡¨: {len(models.get('data', []))} ä¸ªæ¨¡å‹")
                
                # æµ‹è¯•é…ç½®è·å–
                response = client.get('/config')
                if response.status_code == 200:
                    config = json.loads(response.data)
                    logger.info(f"  - é…ç½®ä¿¡æ¯: {len(config.get('data', {}).get('models', {}))} ä¸ªé…ç½®æ¨¡å‹")
                
                logger.info("âœ… APIæ¥å£æµ‹è¯•å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ APIæ¥å£æµ‹è¯•å¤±è´¥: {str(e)}")
    
    async def demo_configuration_management(self):
        """æ¼”ç¤ºé…ç½®ç®¡ç†"""
        logger.info("\n" + "="*60)
        logger.info("âš™ï¸ æ¼”ç¤º6: é…ç½®ç®¡ç†")
        logger.info("="*60)
        
        logger.info("ğŸ“‹ å½“å‰é…ç½®ä¿¡æ¯:")
        config = self.config_manager.get_config()
        logger.info(f"  - ç‰ˆæœ¬: {config.version}")
        logger.info(f"  - æœ€å¤§å·¥ä½œçº¿ç¨‹: {config.system.max_workers}")
        logger.info(f"  - æ—¥å¿—çº§åˆ«: {config.system.log_level}")
        logger.info(f"  - æ¨¡å‹æ•°é‡: {len(config.models)}")
        logger.info(f"  - è¯„ä¼°å™¨æ•°é‡: {len(config.evaluators)}")
        
        # æ¼”ç¤ºé…ç½®æ›´æ–°
        logger.info("ğŸ”„ æ¼”ç¤ºé…ç½®æ›´æ–°...")
        updates = {
            "system": {
                "max_workers": 8,
                "log_level": "DEBUG"
            }
        }
        
        success = self.config_manager.update_config(updates)
        if success:
            logger.info("âœ… é…ç½®æ›´æ–°æˆåŠŸ")
            updated_config = self.config_manager.get_config()
            logger.info(f"  - æ–°çš„æœ€å¤§å·¥ä½œçº¿ç¨‹: {updated_config.system.max_workers}")
            logger.info(f"  - æ–°çš„æ—¥å¿—çº§åˆ«: {updated_config.system.log_level}")
        else:
            logger.warning("âš ï¸ é…ç½®æ›´æ–°å¤±è´¥")
        
        # æ¼”ç¤ºæ¨¡å‹é…ç½®ç®¡ç†
        logger.info("ğŸ¤– æ¼”ç¤ºæ¨¡å‹é…ç½®ç®¡ç†...")
        new_model_config = ModelConfig(
            model_id="demo_new_model",
            adapter_type="demo",
            timeout=60,
            max_retries=5
        )
        
        success = self.config_manager.add_model("demo_new_model", new_model_config)
        if success:
            logger.info("âœ… æ–°æ¨¡å‹é…ç½®æ·»åŠ æˆåŠŸ")
            
            # ç§»é™¤æ¼”ç¤ºæ¨¡å‹
            success = self.config_manager.remove_model("demo_new_model")
            if success:
                logger.info("âœ… æ¼”ç¤ºæ¨¡å‹é…ç½®ç§»é™¤æˆåŠŸ")
    
    async def _monitor_evaluation_progress(self, task_id: str, model_name: str = None):
        """ç›‘æ§è¯„ä¼°è¿›åº¦"""
        model_info = f" ({model_name})" if model_name else ""
        logger.info(f"â³ ç›‘æ§è¯„ä¼°è¿›åº¦{model_info}...")
        
        max_wait_time = 30
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            progress = self.evaluation_engine.get_evaluation_progress(task_id)
            
            if progress:
                if progress.status == "completed":
                    logger.info(f"âœ… è¯„ä¼°å®Œæˆ{model_info}")
                    break
                elif progress.status == "failed":
                    logger.error(f"âŒ è¯„ä¼°å¤±è´¥{model_info}")
                    break
                elif progress.status == "running":
                    logger.info(f"ğŸ”„ è¯„ä¼°è¿›è¡Œä¸­{model_info}...")
            
            await asyncio.sleep(1)
        else:
            logger.warning(f"â° è¯„ä¼°ç›‘æ§è¶…æ—¶{model_info}")
    
    async def _monitor_batch_evaluation_progress(self, task_id: str):
        """ç›‘æ§æ‰¹é‡è¯„ä¼°è¿›åº¦"""
        logger.info("â³ ç›‘æ§æ‰¹é‡è¯„ä¼°è¿›åº¦...")
        
        max_wait_time = 60
        start_time = time.time()
        last_progress = 0
        
        while time.time() - start_time < max_wait_time:
            task_status = self.batch_evaluator.get_batch_task_status(task_id)
            
            if task_status:
                if task_status.status == "completed":
                    logger.info("âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ")
                    break
                elif task_status.status == "failed":
                    logger.error("âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥")
                    if task_status.errors:
                        for error in task_status.errors[:3]:
                            logger.error(f"  - {error}")
                    break
                elif task_status.status == "running":
                    current_progress = task_status.processed_samples
                    if current_progress > last_progress:
                        logger.info(f"ğŸ”„ æ‰¹é‡è¯„ä¼°è¿›åº¦: {current_progress}/{task_status.total_samples}")
                        last_progress = current_progress
            
            await asyncio.sleep(2)
        else:
            logger.warning("â° æ‰¹é‡è¯„ä¼°ç›‘æ§è¶…æ—¶")
    
    async def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        logger.info("ğŸ¬ å¼€å§‹ Industry Evaluation System å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
        logger.info("=" * 80)
        
        try:
            # 1. å•æ¨¡å‹è¯„ä¼°
            task_id = await self.demo_single_evaluation()
            
            # 2. æ¨¡å‹å¯¹æ¯”è¯„ä¼°
            await self.demo_model_comparison()
            
            # 3. æ‰¹é‡è¯„ä¼°
            await self.demo_batch_evaluation()
            
            # 4. æŠ¥å‘Šç”Ÿæˆ
            if task_id:
                await self.demo_report_generation(task_id)
            
            # 5. APIæ¥å£
            await self.demo_api_interface()
            
            # 6. é…ç½®ç®¡ç†
            await self.demo_configuration_management()
            
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ‰ å®Œæ•´åŠŸèƒ½æ¼”ç¤ºç»“æŸ")
            logger.info("=" * 80)
            
            # æ˜¾ç¤ºæ¼”ç¤ºæ€»ç»“
            self._show_demo_summary()
            
        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            self.cleanup()
    
    def _show_demo_summary(self):
        """æ˜¾ç¤ºæ¼”ç¤ºæ€»ç»“"""
        logger.info("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
        logger.info("âœ… å·²æ¼”ç¤ºçš„åŠŸèƒ½:")
        logger.info("  1. ğŸ”§ é…ç½®ç®¡ç† - çµæ´»çš„é…ç½®æ–‡ä»¶ç®¡ç†å’Œçƒ­æ›´æ–°")
        logger.info("  2. ğŸ¤– æ¨¡å‹é€‚é…å™¨ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹å’Œå¼‚å¸¸å¤„ç†")
        logger.info("  3. ğŸ“Š è¯„ä¼°å™¨ç³»ç»Ÿ - å¤šç»´åº¦ä¸“ä¸šè¯„ä¼°èƒ½åŠ›")
        logger.info("  4. ğŸ¯ å•æ¨¡å‹è¯„ä¼° - è¯¦ç»†çš„è¯„ä¼°ç»“æœå’Œæ”¹è¿›å»ºè®®")
        logger.info("  5. âš–ï¸ æ¨¡å‹å¯¹æ¯” - å¤šæ¨¡å‹å¹¶è¡Œè¯„ä¼°å’Œå¯¹æ¯”åˆ†æ")
        logger.info("  6. ğŸ“¦ æ‰¹é‡è¯„ä¼° - å¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆå¤„ç†")
        logger.info("  7. ğŸ“„ æŠ¥å‘Šç”Ÿæˆ - ä¸“ä¸šçš„è¯„ä¼°æŠ¥å‘Šè¾“å‡º")
        logger.info("  8. ğŸŒ APIæ¥å£ - RESTful APIå’Œè‡ªåŠ¨æ–‡æ¡£")
        
        logger.info(f"\nğŸ“ æ¼”ç¤ºæ–‡ä»¶ä½ç½®: {self.temp_dir}")
        logger.info("ğŸ’¡ æç¤º: å¯ä»¥æŸ¥çœ‹ä¸´æ—¶ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶å’ŒæŠ¥å‘Šæ–‡ä»¶")
    
    def cleanup(self):
        """æ¸…ç†æ¼”ç¤ºç¯å¢ƒ"""
        try:
            # å…³é—­è¯„ä¼°å¼•æ“
            if hasattr(self, 'evaluation_engine'):
                self.evaluation_engine.shutdown()
            
            logger.info("ğŸ§¹ æ¼”ç¤ºç¯å¢ƒæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Industry Evaluation System - å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºè¡Œä¸šè¯„ä¼°ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š")
    print("â€¢ é…ç½®ç®¡ç†å’Œæ¨¡å‹é€‚é…å™¨")
    print("â€¢ å•æ¨¡å‹è¯„ä¼°å’Œæ¨¡å‹å¯¹æ¯”")
    print("â€¢ æ‰¹é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("â€¢ APIæ¥å£å’Œé…ç½®ç®¡ç†")
    print("=" * 80)
    
    # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
    demo = IndustryEvaluationDemo()
    await demo.run_complete_demo()


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥æ¼”ç¤º
    asyncio.run(main())