#!/usr/bin/env python3
"""
é«˜çº§è¯„ä¼°ç¤ºä¾‹

æ¼”ç¤ºé«˜çº§è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šæ¨¡å‹å¯¹æ¯”ã€åŸºå‡†æµ‹è¯•ã€è´¨é‡åˆ†æç­‰ã€‚
"""

import sys
from pathlib import Path
import time
from datetime import datetime

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from datasets import Dataset
from evaluation import (
    DataSplitter, EvaluationEngine, EvaluationConfig,
    QualityAnalyzer, BenchmarkManager, ExperimentTracker,
    ReportGenerator, MetricsCalculator
)
from evaluation.data_models import ExperimentConfig, BenchmarkConfig


def create_diverse_dataset():
    """åˆ›å»ºå¤šæ ·åŒ–çš„æ•°æ®é›†"""
    data = []
    
    # æ–‡æœ¬ç”Ÿæˆæ•°æ®
    generation_data = [
        {"input": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½", "output": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"},
        {"input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"},
        {"input": "æ·±åº¦å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ å¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸã€‚"},
    ] * 30
    
    # åˆ†ç±»æ•°æ®
    classification_data = [
        {"text": "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„", "label": "positive"},
        {"text": "æœåŠ¡æ€åº¦ä¸é”™ï¼Œä¼šå†æ¬¡è´­ä¹°", "label": "positive"},
        {"text": "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”é«˜", "label": "positive"},
        {"text": "äº§å“æœ‰ç¼ºé™·ï¼Œä¸æ¨è", "label": "negative"},
        {"text": "å®¢æœæ€åº¦å¾ˆå·®", "label": "negative"},
        {"text": "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼", "label": "negative"},
        {"text": "è¿˜å¯ä»¥ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„", "label": "neutral"},
        {"text": "æ™®é€šçš„äº§å“ï¼Œä¸€èˆ¬èˆ¬", "label": "neutral"},
    ] * 25
    
    # é—®ç­”æ•°æ®
    qa_data = [
        {"question": "åŒ—äº¬æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ", "answer": "ä¸­å›½", "context": "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ã€‚"},
        {"question": "ä¸€å¹´æœ‰å¤šå°‘ä¸ªæœˆï¼Ÿ", "answer": "12ä¸ªæœˆ", "context": "ä¸€å¹´é€šå¸¸æœ‰12ä¸ªæœˆã€‚"},
        {"question": "åœ°çƒæœ‰å‡ ä¸ªå«æ˜Ÿï¼Ÿ", "answer": "1ä¸ª", "context": "åœ°çƒåªæœ‰ä¸€ä¸ªå¤©ç„¶å«æ˜Ÿï¼Œå³æœˆçƒã€‚"},
    ] * 35
    
    return {
        "generation": Dataset.from_list(generation_data),
        "classification": Dataset.from_list(classification_data),
        "qa": Dataset.from_list(qa_data)
    }


def create_mock_models():
    """åˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿæ¨¡å‹"""
    import torch
    import random
    
    class MockModel:
        def __init__(self, name, performance_level=0.8):
            self.name = name
            self.performance_level = performance_level
            self.config = type('Config', (), {'is_encoder_decoder': False})()
        
        def generate(self, **kwargs):
            batch_size = kwargs['input_ids'].shape[0]
            seq_len = kwargs.get('max_length', 50)
            return torch.randint(0, 1000, (batch_size, seq_len))
        
        def __call__(self, **kwargs):
            batch_size = kwargs['input_ids'].shape[0]
            # æ ¹æ®æ€§èƒ½æ°´å¹³è°ƒæ•´è¾“å‡º
            logits = torch.randn(batch_size, 3) * self.performance_level
            return type('Output', (), {'logits': logits})()
    
    class MockTokenizer:
        def __init__(self, name):
            self.name = name
            self.pad_token_id = 0
        
        def __call__(self, texts, **kwargs):
            if isinstance(texts, str):
                texts = [texts]
            
            batch_size = len(texts)
            seq_len = kwargs.get('max_length', 50)
            
            return {
                'input_ids': torch.randint(0, 1000, (batch_size, seq_len)),
                'attention_mask': torch.ones(batch_size, seq_len)
            }
        
        def decode(self, token_ids, **kwargs):
            return f"decoded_text_{len(token_ids)}"
        
        def batch_decode(self, batch_token_ids, **kwargs):
            return [f"decoded_text_{i}" for i in range(len(batch_token_ids))]
    
    # åˆ›å»ºä¸åŒæ€§èƒ½æ°´å¹³çš„æ¨¡å‹
    models = [
        {
            "model": MockModel("baseline_model", 0.7),
            "tokenizer": MockTokenizer("baseline_tokenizer"),
            "name": "baseline_model"
        },
        {
            "model": MockModel("improved_model", 0.85),
            "tokenizer": MockTokenizer("improved_tokenizer"),
            "name": "improved_model"
        },
        {
            "model": MockModel("advanced_model", 0.9),
            "tokenizer": MockTokenizer("advanced_tokenizer"),
            "name": "advanced_model"
        }
    ]
    
    return models


def demonstrate_quality_analysis(datasets):
    """æ¼”ç¤ºè´¨é‡åˆ†æåŠŸèƒ½"""
    print("\nğŸ” æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æ...")
    
    analyzer = QualityAnalyzer(
        min_length=5,
        max_length=1000,
        language="zh"
    )
    
    # åˆ†æåˆ†ç±»æ•°æ®é›†çš„è´¨é‡
    quality_report = analyzer.analyze_data_quality(
        datasets["classification"], 
        text_field="text"
    )
    
    print(f"æ•°æ®é›†æ€»æ ·æœ¬æ•°: {quality_report.total_samples}")
    print(f"è´¨é‡åˆ†æ•°: {quality_report.quality_score:.3f}")
    print(f"å‘ç°çš„é—®é¢˜æ•°: {len(quality_report.quality_issues)}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    analyzer.generate_quality_report(
        quality_report,
        "output/advanced_example_quality_report.html",
        format="html"
    )
    print("è´¨é‡åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: output/advanced_example_quality_report.html")
    
    # è·å–æ”¹è¿›å»ºè®®
    suggestions = analyzer.suggest_improvements(quality_report)
    print("æ”¹è¿›å»ºè®®:")
    for suggestion in suggestions[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
        print(f"  - {suggestion}")
    
    return quality_report


def demonstrate_multi_model_evaluation(models, datasets):
    """æ¼”ç¤ºå¤šæ¨¡å‹è¯„ä¼°"""
    print("\nğŸ† æ‰§è¡Œå¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
    
    config = EvaluationConfig(
        tasks=["classification"],
        metrics=["accuracy", "precision", "recall", "f1"],
        batch_size=4,
        num_samples=100
    )
    
    engine = EvaluationEngine(config, max_workers=2)
    
    # æ¨¡æ‹Ÿæ¨ç†å‡½æ•°
    def create_mock_inference(performance_level):
        def mock_inference(inputs):
            import random
            labels = ["positive", "negative", "neutral"]
            # æ ¹æ®æ€§èƒ½æ°´å¹³è°ƒæ•´å‡†ç¡®ç‡
            correct_rate = performance_level
            results = []
            for _ in inputs:
                if random.random() < correct_rate:
                    # è¿”å›"æ­£ç¡®"ç­”æ¡ˆï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    results.append(random.choice(labels))
                else:
                    # è¿”å›"é”™è¯¯"ç­”æ¡ˆ
                    results.append(random.choice(labels))
            return results
        return mock_inference
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®ä¸åŒçš„æ¨ç†å‡½æ•°
    results = []
    for model_info in models:
        model = model_info["model"]
        performance = model.performance_level
        
        # ä¸´æ—¶æ›¿æ¢æ¨ç†å‡½æ•°
        original_create_func = engine._create_inference_function
        engine._create_inference_function = lambda m, t: create_mock_inference(performance)
        
        result = engine.evaluate_model(
            model=model,
            tokenizer=model_info["tokenizer"],
            datasets={"classification": datasets["classification"]},
            model_name=model_info["name"]
        )
        results.append(result)
        
        print(f"{model_info['name']} - å‡†ç¡®ç‡: {result.metrics.get('accuracy', 0):.3f}")
        
        # æ¢å¤åŸå‡½æ•°
        engine._create_inference_function = original_create_func
    
    return results


def demonstrate_benchmark_evaluation(models):
    """æ¼”ç¤ºåŸºå‡†æµ‹è¯•è¯„ä¼°"""
    print("\nğŸ“Š æ‰§è¡ŒåŸºå‡†æµ‹è¯•è¯„ä¼°...")
    
    # åˆ›å»ºè‡ªå®šä¹‰åŸºå‡†é…ç½®
    custom_benchmark_config = BenchmarkConfig(
        name="sentiment_benchmark",
        dataset_path="custom_sentiment_data.json",
        tasks=["sentiment_classification"],
        evaluation_protocol="standard",
        metrics=["accuracy", "f1"]
    )
    
    # æ¨¡æ‹ŸåŸºå‡†ç®¡ç†å™¨
    class MockBenchmarkManager:
        def __init__(self):
            pass
        
        def list_available_benchmarks(self):
            return ["clue", "few_clue", "c_eval", "custom_sentiment"]
        
        def run_custom_benchmark(self, config, model, tokenizer, model_name):
            # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ç»“æœ
            import random
            from evaluation.data_models import BenchmarkResult
            
            task_results = {}
            for task in config.tasks:
                task_results[task] = {
                    "accuracy": random.uniform(0.7, 0.95),
                    "f1": random.uniform(0.65, 0.9)
                }
            
            overall_score = sum(
                sum(scores.values()) / len(scores) 
                for scores in task_results.values()
            ) / len(task_results)
            
            return BenchmarkResult(
                benchmark_name=config.name,
                model_name=model_name,
                task_results=task_results,
                overall_score=overall_score,
                metadata={"custom": True},
                evaluation_time=datetime.now()
            )
    
    benchmark_manager = MockBenchmarkManager()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = []
    for model_info in models:
        result = benchmark_manager.run_custom_benchmark(
            custom_benchmark_config,
            model_info["model"],
            model_info["tokenizer"],
            model_info["name"]
        )
        benchmark_results.append(result)
        
        print(f"{model_info['name']} - åŸºå‡†åˆ†æ•°: {result.overall_score:.3f}")
    
    return benchmark_results


def demonstrate_advanced_reporting(evaluation_results, benchmark_results):
    """æ¼”ç¤ºé«˜çº§æŠ¥å‘Šç”Ÿæˆ"""
    print("\nğŸ“ˆ ç”Ÿæˆé«˜çº§æŠ¥å‘Š...")
    
    generator = ReportGenerator(
        output_dir="output/advanced_example_reports",
        include_plots=True,
        language="zh"
    )
    
    # ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
    comparison_report = generator.generate_comparison_report(
        evaluation_results,
        format="html"
    )
    print(f"å¯¹æ¯”æŠ¥å‘Š: {comparison_report}")
    
    # ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
    for benchmark_result in benchmark_results:
        benchmark_report = generator.generate_benchmark_report(
            benchmark_result,
            format="html"
        )
        print(f"åŸºå‡†æŠ¥å‘Š ({benchmark_result.model_name}): {benchmark_report}")
    
    # ç”ŸæˆLaTeXè¡¨æ ¼
    latex_table = generator.generate_latex_table(
        evaluation_results,
        metrics=["accuracy", "f1"]
    )
    print(f"LaTeXè¡¨æ ¼: {latex_table}")
    
    # ç”ŸæˆCSVå¯¼å‡º
    csv_export = generator.generate_csv_export(evaluation_results)
    print(f"CSVå¯¼å‡º: {csv_export}")
    
    return {
        "comparison_report": comparison_report,
        "latex_table": latex_table,
        "csv_export": csv_export
    }


def demonstrate_experiment_analysis(evaluation_results, tracker):
    """æ¼”ç¤ºå®éªŒåˆ†æåŠŸèƒ½"""
    print("\nğŸ“Š æ‰§è¡Œå®éªŒåˆ†æ...")
    
    # è·Ÿè¸ªæ‰€æœ‰å®éªŒ
    experiment_ids = []
    for i, result in enumerate(evaluation_results):
        experiment_config = ExperimentConfig(
            model_name=result.model_name,
            dataset_name="advanced_example_dataset",
            hyperparameters={
                "batch_size": 4,
                "num_samples": 100,
                "model_version": f"v1.{i}"
            },
            tags=["advanced_example", f"model_{i}"],
            description=f"é«˜çº§ç¤ºä¾‹å®éªŒ - {result.model_name}"
        )
        
        exp_id = tracker.track_experiment(experiment_config, result)
        experiment_ids.append(exp_id)
    
    # ç”Ÿæˆæ’è¡Œæ¦œ
    leaderboard = tracker.generate_leaderboard(metric="accuracy")
    print("\nğŸ† æ¨¡å‹æ’è¡Œæ¦œ (æŒ‰å‡†ç¡®ç‡):")
    for i, entry in enumerate(leaderboard[:3], 1):
        print(f"  {i}. {entry['model_name']}: {entry['score']:.4f}")
    
    # å¯¹æ¯”å®éªŒ
    comparison = tracker.compare_experiments(experiment_ids)
    print(f"\næœ€ä½³æ¨¡å‹: {comparison['best_model']['model_name']}")
    
    # è·å–å®éªŒç»Ÿè®¡
    stats = tracker.get_experiment_statistics()
    print(f"å®éªŒç»Ÿè®¡:")
    print(f"  æ€»å®éªŒæ•°: {stats['total_experiments']}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {stats.get('avg_accuracy', 'N/A')}")
    print(f"  æœ€ä½³å‡†ç¡®ç‡: {stats.get('best_accuracy', 'N/A')}")
    
    return {
        "leaderboard": leaderboard,
        "comparison": comparison,
        "statistics": stats
    }


def demonstrate_metrics_calculation():
    """æ¼”ç¤ºæŒ‡æ ‡è®¡ç®—åŠŸèƒ½"""
    print("\nğŸ§® æ¼”ç¤ºæŒ‡æ ‡è®¡ç®—...")
    
    calculator = MetricsCalculator(language="zh")
    
    # ç¤ºä¾‹é¢„æµ‹å’Œå‚è€ƒæ–‡æœ¬
    predictions = [
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“",
        "è´¨é‡ä¸é”™ï¼Œæ¨èè´­ä¹°",
        "ä»·æ ¼æœ‰ç‚¹è´µä½†å€¼å¾—"
    ]
    
    references = [
        "è¿™æ˜¯ä¸€ä¸ªä¼˜ç§€çš„äº§å“",
        "è´¨é‡å¾ˆå¥½ï¼Œå€¼å¾—æ¨è",
        "è™½ç„¶ä»·æ ¼è¾ƒé«˜ä½†ç‰©æœ‰æ‰€å€¼"
    ]
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_scores = calculator.calculate_bleu(predictions, references)
    print(f"BLEUåˆ†æ•°: {bleu_scores['bleu']:.4f}")
    
    # è®¡ç®—ROUGEåˆ†æ•°
    rouge_scores = calculator.calculate_rouge(predictions, references)
    print(f"ROUGE-Låˆ†æ•°: {rouge_scores.get('rougeL', 'N/A')}")
    
    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    similarity_scores = calculator.calculate_semantic_similarity(
        predictions, references, method="cosine"
    )
    print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity_scores['cosine_similarity']:.4f}")
    
    # åˆ†ç±»æŒ‡æ ‡ç¤ºä¾‹
    class_predictions = ["positive", "negative", "positive", "neutral"]
    class_references = ["positive", "positive", "negative", "neutral"]
    
    class_metrics = calculator.calculate_classification_metrics(
        class_predictions, class_references
    )
    print(f"åˆ†ç±»å‡†ç¡®ç‡: {class_metrics['accuracy']:.4f}")
    print(f"F1åˆ†æ•°: {class_metrics['f1']:.4f}")
    
    return {
        "bleu": bleu_scores,
        "rouge": rouge_scores,
        "similarity": similarity_scores,
        "classification": class_metrics
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é«˜çº§è¯„ä¼°ç¤ºä¾‹...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    Path("output/advanced_example_reports").mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºå¤šæ ·åŒ–æ•°æ®é›†
    print("\nğŸ“Š åˆ›å»ºå¤šæ ·åŒ–æ•°æ®é›†...")
    datasets = create_diverse_dataset()
    print(f"ç”Ÿæˆæ•°æ®é›†: {len(datasets)} ä¸ªä»»åŠ¡")
    for task, dataset in datasets.items():
        print(f"  {task}: {len(dataset)} ä¸ªæ ·æœ¬")
    
    # 2. æ•°æ®è´¨é‡åˆ†æ
    quality_report = demonstrate_quality_analysis(datasets)
    
    # 3. åˆ›å»ºå¤šä¸ªæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿæ¨¡å‹...")
    models = create_mock_models()
    print(f"åˆ›å»ºäº† {len(models)} ä¸ªæ¨¡å‹")
    
    # 4. å¤šæ¨¡å‹è¯„ä¼°
    evaluation_results = demonstrate_multi_model_evaluation(models, datasets)
    
    # 5. åŸºå‡†æµ‹è¯•è¯„ä¼°
    benchmark_results = demonstrate_benchmark_evaluation(models)
    
    # 6. æŒ‡æ ‡è®¡ç®—æ¼”ç¤º
    metrics_demo = demonstrate_metrics_calculation()
    
    # 7. å®éªŒè·Ÿè¸ªå’Œåˆ†æ
    tracker = ExperimentTracker(
        experiment_dir="output/advanced_example_experiments"
    )
    experiment_analysis = demonstrate_experiment_analysis(evaluation_results, tracker)
    
    # 8. é«˜çº§æŠ¥å‘Šç”Ÿæˆ
    reports = demonstrate_advanced_reporting(evaluation_results, benchmark_results)
    
    # 9. æ€§èƒ½åˆ†æ
    print("\nâš¡ æ€§èƒ½åˆ†æ...")
    from evaluation.efficiency_analyzer import EfficiencyAnalyzer
    
    try:
        efficiency_analyzer = EfficiencyAnalyzer()
        
        # æ¨¡æ‹Ÿæ€§èƒ½æµ‹é‡
        def mock_inference_func(inputs):
            time.sleep(0.01)  # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
            return ["result"] * len(inputs)
        
        latency = efficiency_analyzer.measure_inference_latency(
            mock_inference_func,
            ["test"] * 10,
            batch_size=2
        )
        print(f"å¹³å‡æ¨ç†å»¶è¿Ÿ: {latency:.3f}ms")
        
    except ImportError:
        print("æ•ˆç‡åˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½åˆ†æ")
    
    # 10. æ€»ç»“
    print("\nâœ… é«˜çº§è¯„ä¼°ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - è´¨é‡åˆ†ææŠ¥å‘Š: output/advanced_example_quality_report.html")
    print("  - è¯„ä¼°æŠ¥å‘Š: output/advanced_example_reports/")
    print("  - å®éªŒè®°å½•: output/advanced_example_experiments/")
    print("  - ç»“æœå¯¼å‡º: output/advanced_example_reports/")
    
    print("\nğŸ“ˆ å…³é”®ç»“æœ:")
    print(f"  - æœ€ä½³æ¨¡å‹: {experiment_analysis['comparison']['best_model']['model_name']}")
    print(f"  - æ•°æ®è´¨é‡åˆ†æ•°: {quality_report.quality_score:.3f}")
    print(f"  - æ€»å®éªŒæ•°: {experiment_analysis['statistics']['total_experiments']}")
    
    return {
        "datasets": datasets,
        "models": models,
        "evaluation_results": evaluation_results,
        "benchmark_results": benchmark_results,
        "quality_report": quality_report,
        "experiment_analysis": experiment_analysis,
        "reports": reports,
        "metrics_demo": metrics_demo
    }


if __name__ == "__main__":
    try:
        results = main()
        print("\nğŸ‰ æ‰€æœ‰é«˜çº§åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ é«˜çº§ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)