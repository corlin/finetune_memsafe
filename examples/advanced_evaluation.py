#!/usr/bin/env python3
"""
é«˜çº§è¯„ä¼°ç¤ºä¾‹

æ¼”ç¤ºé«˜çº§è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šæ¨¡å‹å¯¹æ¯”ã€åŸºå‡†æµ‹è¯•ã€è´¨é‡åˆ†æç­‰ã€‚
"""

import sys
from pathlib import Path
import time
from datetime import datetime

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from datasets import Dataset
from evaluation import (
    DataSplitter, EvaluationEngine, EvaluationConfig,
    QualityAnalyzer, BenchmarkManager, ExperimentTracker,
    ReportGenerator, MetricsCalculator
)
from evaluation.data_models import ExperimentConfig, BenchmarkConfig


def create_diverse_dataset():
    """åˆ›å»ºå¤šæ ·åŒ–çš„æ•°æ®é›†"""
    data = []
    
    # æ–‡æœ¬ç”Ÿæˆæ•°æ®
    generation_data = [
        {"input": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½", "output": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"},
        {"input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"},
        {"input": "æ·±åº¦å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ å¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸã€‚"},
    ] * 30
    
    # åˆ†ç±»æ•°æ®
    classification_data = [
        {"text": "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„", "label": "positive"},
        {"text": "æœåŠ¡æ€åº¦ä¸é”™ï¼Œä¼šå†æ¬¡è´­ä¹°", "label": "positive"},
        {"text": "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”é«˜", "label": "positive"},
        {"text": "äº§å“æœ‰ç¼ºé™·ï¼Œä¸æ¨è", "label": "negative"},
        {"text": "å®¢æœæ€åº¦å¾ˆå·®", "label": "negative"},
        {"text": "å®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼", "label": "negative"},
        {"text": "è¿˜å¯ä»¥ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„", "label": "neutral"},
        {"text": "æ™®é€šçš„äº§å“ï¼Œä¸€èˆ¬èˆ¬", "label": "neutral"},
    ] * 25
    
    # é—®ç­”æ•°æ®
    qa_data = [
        {"question": "åŒ—äº¬æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ", "answer": "ä¸­å›½", "context": "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ã€‚"},
        {"question": "ä¸€å¹´æœ‰å¤šå°‘ä¸ªæœˆï¼Ÿ", "answer": "12ä¸ªæœˆ", "context": "ä¸€å¹´é€šå¸¸æœ‰12ä¸ªæœˆã€‚"},
        {"question": "åœ°çƒæœ‰å‡ ä¸ªå«æ˜Ÿï¼Ÿ", "answer": "1ä¸ª", "context": "åœ°çƒåªæœ‰ä¸€ä¸ªå¤©ç„¶å«æ˜Ÿï¼Œå³æœˆçƒã€‚"},
    ] * 35
    
    return {
        "generation": Dataset.from_list(generation_data),
        "classification": Dataset.from_list(classification_data),
        "qa": Dataset.from_list(qa_data)
    }


def create_mock_models():
    """åˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿæ¨¡å‹"""
    import torch
    import random
    
    class MockModel:
        def __init__(self, name, performance_level=0.8):
            self.name = name
            self.performance_level = performance_level
            self.config = type('Config', (), {'is_encoder_decoder': False})()
        
        def generate(self, **kwargs):
            batch_size = kwargs['input_ids'].shape[0]
            seq_len = kwargs.get('max_length', 50)
            return torch.randint(0, 1000, (batch_size, seq_len))
        
        def __call__(self, **kwargs):
            batch_size = kwargs['input_ids'].shape[0]
            # æ ¹æ®æ€§èƒ½æ°´å¹³è°ƒæ•´è¾“å‡º
            logits = torch.randn(batch_size, 3) * self.performance_level
            return type('Output', (), {'logits': logits})()
    
    class MockTokenizer:
        def __init__(self, name):
            self.name = name
            self.pad_token_id = 0
        
        def __call__(self, texts, **kwargs):
            if isinstance(texts, str):
                texts = [texts]
            
            batch_size = len(texts)
            seq_len = kwargs.get('max_length', 50)
            
            return {
                'input_ids': torch.randint(0, 1000, (batch_size, seq_len)),
                'attention_mask': torch.ones(batch_size, seq_len)
            }
        
        def decode(self, token_ids, **kwargs):
            return f"decoded_text_{len(token_ids)}"
        
        def batch_decode(self, batch_token_ids, **kwargs):
            return [f"decoded_text_{i}" for i in range(len(batch_token_ids))]
    
    # åˆ›å»ºä¸åŒæ€§èƒ½æ°´å¹³çš„æ¨¡å‹
    models = [
        {
            "model": MockModel("baseline_model", 0.7),
            "tokenizer": MockTokenizer("baseline_tokenizer"),
            "name": "baseline_model"
        },
        {
            "model": MockModel("improved_model", 0.85),
            "tokenizer": MockTokenizer("improved_tokenizer"),
            "name": "improved_model"
        },
        {
            "model": MockModel("advanced_model", 0.9),
            "tokenizer": MockTokenizer("advanced_tokenizer"),
            "name": "advanced_model"
        }
    ]
    
    return models


def demonstrate_quality_analysis(datasets):
    """æ¼”ç¤ºè´¨é‡åˆ†æåŠŸèƒ½"""
    print("\nğŸ” æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æ...")
    
    analyzer = QualityAnalyzer(
        min_length=5,
        max_length=1000
    )
    
    # åˆ†æåˆ†ç±»æ•°æ®é›†çš„è´¨é‡
    quality_report = analyzer.analyze_data_quality(
        datasets["classification"], 
        dataset_name="classification_dataset"
    )
    
    print(f"æ•°æ®é›†æ€»æ ·æœ¬æ•°: {quality_report.total_samples}")
    print(f"è´¨é‡åˆ†æ•°: {quality_report.quality_score:.3f}")
    print(f"å‘ç°çš„é—®é¢˜æ•°: {len(quality_report.issues)}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    analyzer.generate_quality_report(
        quality_report,
        "output"
    )
    print("è´¨é‡åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: output/advanced_example_quality_report.html")
    
    # è·å–æ”¹è¿›å»ºè®®
    suggestions = analyzer.suggest_improvements(quality_report.statistics, quality_report.issues)
    print("æ”¹è¿›å»ºè®®:")
    for suggestion in suggestions[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
        print(f"  - {suggestion}")
    
    return quality_report


def demonstrate_multi_model_evaluation(models, datasets):
    """æ¼”ç¤ºå¤šæ¨¡å‹è¯„ä¼°"""
    print("\nğŸ† æ‰§è¡Œå¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
    
    # ç®€åŒ–çš„æ¨¡æ‹Ÿè¯„ä¼°ï¼Œé¿å…å¤æ‚çš„æ¨ç†è¿‡ç¨‹
    results = []
    
    for model_info in models:
        model = model_info["model"]
        performance = model.performance_level
        
        # ç›´æ¥åˆ›å»ºæ¨¡æ‹Ÿçš„è¯„ä¼°ç»“æœ
        from evaluation.data_models import EvaluationResult, TaskResult, EvaluationSample, EfficiencyMetrics, QualityScores, EvaluationConfig
        import random
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„ä»»åŠ¡ç»“æœ
        samples = [
            EvaluationSample(
                input_text=f"sample_{i}",
                reference="positive",
                prediction=random.choice(["positive", "negative", "neutral"]),
                metrics={"accuracy": random.uniform(0.7, 0.95)}
            ) for i in range(50)
        ]
        
        task_result = TaskResult(
            task_name="classification",
            predictions=[s.prediction for s in samples],
            references=[s.reference for s in samples],
            metrics={
                "accuracy": performance + random.uniform(-0.1, 0.1),
                "precision": performance + random.uniform(-0.05, 0.05),
                "recall": performance + random.uniform(-0.05, 0.05),
                "f1": performance + random.uniform(-0.05, 0.05)
            },
            samples=samples,
            execution_time=random.uniform(1.0, 3.0)
        )
        
        # åˆ›å»ºè¯„ä¼°ç»“æœ
        result = EvaluationResult(
            model_name=model_info["name"],
            evaluation_time=datetime.now(),
            metrics=task_result.metrics,
            task_results={"classification": task_result},
            efficiency_metrics=EfficiencyMetrics(
                inference_latency=random.uniform(10, 50),
                throughput=random.uniform(100, 500),
                memory_usage=random.uniform(1, 4),
                model_size=random.uniform(100, 1000)
            ),
            quality_scores=QualityScores(
                fluency=random.uniform(0.8, 0.95),
                coherence=random.uniform(0.8, 0.95),
                relevance=random.uniform(0.8, 0.95),
                factuality=random.uniform(0.8, 0.95),
                overall=random.uniform(0.8, 0.95)
            ),
            config=EvaluationConfig(
                tasks=["classification"],
                metrics=["accuracy", "precision", "recall", "f1"],
                batch_size=4,
                num_samples=50
            )
        )
        
        results.append(result)
        print(f"{model_info['name']} - å‡†ç¡®ç‡: {result.metrics.get('accuracy', 0):.3f}")
    
    return results


def demonstrate_benchmark_evaluation(models):
    """æ¼”ç¤ºåŸºå‡†æµ‹è¯•è¯„ä¼°"""
    print("\nğŸ“Š æ‰§è¡ŒåŸºå‡†æµ‹è¯•è¯„ä¼°...")
    
    # åˆ›å»ºè‡ªå®šä¹‰åŸºå‡†é…ç½®
    custom_benchmark_config = BenchmarkConfig(
        name="sentiment_benchmark",
        dataset_path="custom_sentiment_data.json",
        tasks=["sentiment_classification"],
        evaluation_protocol="standard",
        metrics=["accuracy", "f1"]
    )
    
    # æ¨¡æ‹ŸåŸºå‡†ç®¡ç†å™¨
    class MockBenchmarkManager:
        def __init__(self):
            pass
        
        def list_available_benchmarks(self):
            return ["clue", "few_clue", "c_eval", "custom_sentiment"]
        
        def run_custom_benchmark(self, config, model, tokenizer, model_name):
            # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ç»“æœ
            import random
            from evaluation.data_models import BenchmarkResult, TaskResult, EvaluationSample
            
            task_results = {}
            for task in config.tasks:
                # åˆ›å»ºæ¨¡æ‹Ÿçš„è¯„ä¼°æ ·æœ¬
                samples = [
                    EvaluationSample(
                        input_text=f"sample_{i}",
                        reference=f"ref_{i}",
                        prediction=f"pred_{i}",
                        metrics={"accuracy": random.uniform(0.7, 0.95)}
                    ) for i in range(10)
                ]
                
                # åˆ›å»ºTaskResultå¯¹è±¡
                task_result = TaskResult(
                    task_name=task,
                    predictions=[f"pred_{i}" for i in range(10)],
                    references=[f"ref_{i}" for i in range(10)],
                    metrics={
                        "accuracy": random.uniform(0.7, 0.95),
                        "f1": random.uniform(0.65, 0.9)
                    },
                    samples=samples,
                    execution_time=random.uniform(1.0, 5.0)
                )
                task_results[task] = task_result
            
            overall_score = sum(
                sum(task_result.metrics.values()) / len(task_result.metrics) 
                for task_result in task_results.values()
            ) / len(task_results)
            
            return BenchmarkResult(
                benchmark_name=config.name,
                model_name=model_name,
                task_results=task_results,
                overall_score=overall_score,
                ranking_info={"custom": True},
                evaluation_time=datetime.now()
            )
    
    benchmark_manager = MockBenchmarkManager()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = []
    for model_info in models:
        result = benchmark_manager.run_custom_benchmark(
            custom_benchmark_config,
            model_info["model"],
            model_info["tokenizer"],
            model_info["name"]
        )
        benchmark_results.append(result)
        
        print(f"{model_info['name']} - åŸºå‡†åˆ†æ•°: {result.overall_score:.3f}")
    
    return benchmark_results


def demonstrate_advanced_reporting(evaluation_results, benchmark_results):
    """æ¼”ç¤ºé«˜çº§æŠ¥å‘Šç”Ÿæˆ"""
    print("\nğŸ“ˆ ç”Ÿæˆé«˜çº§æŠ¥å‘Š...")
    
    generator = ReportGenerator(
        output_dir="output/advanced_example_reports"
    )
    
    # åˆ›å»ºæ¯”è¾ƒç»“æœå¯¹è±¡
    from evaluation.data_models import ComparisonResult
    
    # æå–æ¨¡å‹åç§°å’ŒæŒ‡æ ‡
    models = [result.model_name for result in evaluation_results]
    metrics = {}
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    all_metric_names = set()
    for result in evaluation_results:
        all_metric_names.update(result.metrics.keys())
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„å€¼
    for metric_name in all_metric_names:
        metric_values = []
        for result in evaluation_results:
            metric_value = result.metrics.get(metric_name, 0.0)
            metric_values.append(float(metric_value))
        metrics[metric_name] = metric_values
    
    # è®¡ç®—æ’å
    rankings = {}
    for metric_name, values in metrics.items():
        # æŒ‰å€¼æ’åºï¼ˆé™åºï¼‰
        sorted_indices = sorted(range(len(values)), key=lambda i: values[i], reverse=True)
        rankings[metric_name] = [models[i] for i in sorted_indices]
    
    # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
    best_model = {}
    for metric_name, model_list in rankings.items():
        if model_list:
            best_model[metric_name] = model_list[0]
    
    # åˆ›å»ºæ¯”è¾ƒç»“æœå¯¹è±¡
    comparison_result = ComparisonResult(
        models=models,
        metrics=metrics,
        statistical_tests={
            "comparison_type": "descriptive",
            "num_models": len(models),
            "metrics_compared": list(all_metric_names)
        },
        rankings=rankings,
        best_model=best_model
    )
    
    # ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
    comparison_report = generator.generate_comparison_report(
        comparison_result,
        format_type="html"
    )
    print(f"å¯¹æ¯”æŠ¥å‘Š: {comparison_report}")
    
    # ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
    for benchmark_result in benchmark_results:
        try:
            benchmark_report = generator.generate_benchmark_report(
                benchmark_result,
                format_type="html"
            )
            print(f"åŸºå‡†æŠ¥å‘Š ({benchmark_result.model_name}): {benchmark_report}")
        except Exception as e:
            print(f"åŸºå‡†æŠ¥å‘Š ({benchmark_result.model_name}): ç”Ÿæˆå¤±è´¥ - {e}")
    
    # ç”ŸæˆLaTeXè¡¨æ ¼
    latex_table = generator.generate_latex_table(
        comparison_result,
        caption="æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ",
        label="tab:model_comparison"
    )
    print(f"LaTeXè¡¨æ ¼: {latex_table}")
    
    # ç”ŸæˆCSVå¯¼å‡º
    csv_export = generator._generate_csv_report(evaluation_results[0], "export")  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ
    print(f"CSVå¯¼å‡º: {csv_export}")
    
    return {
        "comparison_report": comparison_report,
        "latex_table": latex_table,
        "csv_export": csv_export
    }


def demonstrate_experiment_analysis(evaluation_results, tracker):
    """æ¼”ç¤ºå®éªŒåˆ†æåŠŸèƒ½"""
    print("\nğŸ“Š æ‰§è¡Œå®éªŒåˆ†æ...")
    
    # è·Ÿè¸ªæ‰€æœ‰å®éªŒ
    experiment_ids = []
    for i, result in enumerate(evaluation_results):
        experiment_config = ExperimentConfig(
            experiment_name=f"advanced_example_{i}",
            model_config={"model_name": result.model_name},
            training_config={},
            evaluation_config=EvaluationConfig(
                tasks=["classification"],
                metrics=["accuracy"],
                batch_size=4,
                num_samples=100
            ),
            data_config={"dataset_name": "advanced_example_dataset"},
            tags=["advanced_example", f"model_{i}"],
            description=f"é«˜çº§ç¤ºä¾‹å®éªŒ - {result.model_name}"
        )
        
        exp_id = tracker.track_experiment(experiment_config, result)
        experiment_ids.append(exp_id)
    
    # ç”Ÿæˆæ’è¡Œæ¦œ
    leaderboard = tracker.generate_leaderboard(metric="accuracy")
    print("\nğŸ† æ¨¡å‹æ’è¡Œæ¦œ (æŒ‰å‡†ç¡®ç‡):")
    for i, entry in enumerate(leaderboard[:3], 1):
        print(f"  {i}. {entry['model_name']}: {entry['score']:.4f}")
    
    # å¯¹æ¯”å®éªŒ
    comparison = tracker.compare_experiments(experiment_ids)
    best_model = comparison.best_model.get("accuracy", "Unknown") if comparison.best_model else "Unknown"
    print(f"\næœ€ä½³æ¨¡å‹: {best_model}")
    
    # è·å–å®éªŒç»Ÿè®¡
    stats = tracker.get_experiment_statistics()
    print(f"å®éªŒç»Ÿè®¡:")
    print(f"  æ€»å®éªŒæ•°: {stats['total_experiments']}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {stats.get('avg_accuracy', 'N/A')}")
    print(f"  æœ€ä½³å‡†ç¡®ç‡: {stats.get('best_accuracy', 'N/A')}")
    
    return {
        "leaderboard": leaderboard,
        "comparison": comparison,
        "statistics": stats
    }


def demonstrate_metrics_calculation():
    """æ¼”ç¤ºæŒ‡æ ‡è®¡ç®—åŠŸèƒ½"""
    print("\nğŸ§® æ¼”ç¤ºæŒ‡æ ‡è®¡ç®—...")
    
    calculator = MetricsCalculator(language="zh")
    
    # ç¤ºä¾‹é¢„æµ‹å’Œå‚è€ƒæ–‡æœ¬
    predictions = [
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„äº§å“",
        "è´¨é‡ä¸é”™ï¼Œæ¨èè´­ä¹°",
        "ä»·æ ¼æœ‰ç‚¹è´µä½†å€¼å¾—"
    ]
    
    references = [
        "è¿™æ˜¯ä¸€ä¸ªä¼˜ç§€çš„äº§å“",
        "è´¨é‡å¾ˆå¥½ï¼Œå€¼å¾—æ¨è",
        "è™½ç„¶ä»·æ ¼è¾ƒé«˜ä½†ç‰©æœ‰æ‰€å€¼"
    ]
    
    # è®¡ç®—BLEUåˆ†æ•°
    bleu_scores = calculator.calculate_bleu(predictions, references)
    print(f"BLEUåˆ†æ•°: {bleu_scores['bleu']:.4f}")
    
    # è®¡ç®—ROUGEåˆ†æ•°
    rouge_scores = calculator.calculate_rouge(predictions, references)
    print(f"ROUGE-Låˆ†æ•°: {rouge_scores.get('rougeL', 'N/A')}")
    
    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    similarity_scores = calculator.calculate_semantic_similarity(
        predictions, references, method="cosine"
    )
    print(f"è¯­ä¹‰ç›¸ä¼¼åº¦: {similarity_scores['cosine_similarity']:.4f}")
    
    # åˆ†ç±»æŒ‡æ ‡ç¤ºä¾‹
    class_predictions = ["positive", "negative", "positive", "neutral"]
    class_references = ["positive", "positive", "negative", "neutral"]
    
    class_metrics = calculator.calculate_classification_metrics(
        class_predictions, class_references
    )
    print(f"åˆ†ç±»å‡†ç¡®ç‡: {class_metrics['accuracy']:.4f}")
    print(f"F1åˆ†æ•°: {class_metrics['f1']:.4f}")
    
    return {
        "bleu": bleu_scores,
        "rouge": rouge_scores,
        "similarity": similarity_scores,
        "classification": class_metrics
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é«˜çº§è¯„ä¼°ç¤ºä¾‹...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path("output").mkdir(exist_ok=True)
    Path("output/advanced_example_reports").mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºå¤šæ ·åŒ–æ•°æ®é›†
    print("\nğŸ“Š åˆ›å»ºå¤šæ ·åŒ–æ•°æ®é›†...")
    datasets = create_diverse_dataset()
    print(f"ç”Ÿæˆæ•°æ®é›†: {len(datasets)} ä¸ªä»»åŠ¡")
    for task, dataset in datasets.items():
        print(f"  {task}: {len(dataset)} ä¸ªæ ·æœ¬")
    
    # 2. æ•°æ®è´¨é‡åˆ†æ
    quality_report = demonstrate_quality_analysis(datasets)
    
    # 3. åˆ›å»ºå¤šä¸ªæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºå¤šä¸ªæ¨¡æ‹Ÿæ¨¡å‹...")
    models = create_mock_models()
    print(f"åˆ›å»ºäº† {len(models)} ä¸ªæ¨¡å‹")
    
    # 4. å¤šæ¨¡å‹è¯„ä¼°
    evaluation_results = demonstrate_multi_model_evaluation(models, datasets)
    
    # 5. åŸºå‡†æµ‹è¯•è¯„ä¼°
    benchmark_results = demonstrate_benchmark_evaluation(models)
    
    # 6. æŒ‡æ ‡è®¡ç®—æ¼”ç¤º
    metrics_demo = demonstrate_metrics_calculation()
    
    # 7. å®éªŒè·Ÿè¸ªå’Œåˆ†æ
    tracker = ExperimentTracker(
        experiment_dir="output/advanced_example_experiments"
    )
    experiment_analysis = demonstrate_experiment_analysis(evaluation_results, tracker)
    
    # 8. é«˜çº§æŠ¥å‘Šç”Ÿæˆ
    reports = demonstrate_advanced_reporting(evaluation_results, benchmark_results)
    
    # 9. æ€§èƒ½åˆ†æ
    print("\nâš¡ æ€§èƒ½åˆ†æ...")
    from evaluation.efficiency_analyzer import EfficiencyAnalyzer
    
    try:
        efficiency_analyzer = EfficiencyAnalyzer()
        
        # æ¨¡æ‹Ÿæ€§èƒ½æµ‹é‡
        def mock_inference_func(inputs):
            time.sleep(0.01)  # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
            return ["result"] * len(inputs)
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•æµ‹é‡å»¶è¿Ÿå’Œååé‡
        latency_results = efficiency_analyzer.measure_latency_and_throughput(
            mock_inference_func,
            ["test"] * 10,
            batch_sizes=[1, 2, 4],
            num_runs=3
        )
        
        if "overall" in latency_results:
            avg_latency = latency_results["overall"]["avg_latency_per_sample_ms"]
            print(f"å¹³å‡æ¨ç†å»¶è¿Ÿ: {avg_latency:.3f}ms")
        else:
            print("æ€§èƒ½æµ‹é‡å®Œæˆ")
        
    except ImportError:
        print("æ•ˆç‡åˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½åˆ†æ")
    except Exception as e:
        print(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
    
    # 10. æ€»ç»“
    print("\nâœ… é«˜çº§è¯„ä¼°ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - è´¨é‡åˆ†ææŠ¥å‘Š: output/advanced_example_quality_report.html")
    print("  - è¯„ä¼°æŠ¥å‘Š: output/advanced_example_reports/")
    print("  - å®éªŒè®°å½•: output/advanced_example_experiments/")
    print("  - ç»“æœå¯¼å‡º: output/advanced_example_reports/")
    
    print("\nğŸ“ˆ å…³é”®ç»“æœ:")
    best_model_name = experiment_analysis['comparison'].best_model.get("accuracy", "Unknown") if experiment_analysis['comparison'].best_model else "Unknown"
    print(f"  - æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"  - æ•°æ®è´¨é‡åˆ†æ•°: {quality_report.quality_score:.3f}")
    print(f"  - æ€»å®éªŒæ•°: {experiment_analysis['statistics']['total_experiments']}")
    
    return {
        "datasets": datasets,
        "models": models,
        "evaluation_results": evaluation_results,
        "benchmark_results": benchmark_results,
        "quality_report": quality_report,
        "experiment_analysis": experiment_analysis,
        "reports": reports,
        "metrics_demo": metrics_demo
    }


if __name__ == "__main__":
    try:
        results = main()
        print("\nğŸ‰ æ‰€æœ‰é«˜çº§åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ é«˜çº§ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
