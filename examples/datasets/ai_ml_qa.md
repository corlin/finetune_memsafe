# 人工智能与机器学习问答数据集

## 机器学习基础

Q1: 什么是过拟合？如何避免？
A1: 过拟合是指模型在训练数据上表现很好，但在新数据上表现差的现象。避免方法包括：增加训练数据、使用正则化技术、简化模型复杂度、交叉验证、早停法等。

Q2: 偏差和方差的权衡是什么？
A2: 偏差-方差权衡描述了模型复杂度与预测误差的关系。高偏差（欠拟合）意味着模型过于简单，高方差（过拟合）意味着模型过于复杂。目标是找到最优平衡点，最小化总误差。

Q3: 什么是交叉验证？
A3: 交叉验证是评估模型性能的技术，将数据分为k个子集，轮流使用k-1个子集训练，1个子集验证。常用的是5折或10折交叉验证，能更可靠地评估模型泛化能力。

Q4: 梯度下降算法的原理是什么？
A4: 梯度下降通过计算损失函数的梯度，沿着梯度的反方向更新参数，逐步找到最优解。学习率控制更新步长，过大可能跳过最优解，过小收敛太慢。

Q5: 什么是特征工程？
A5: 特征工程是从原始数据中提取、选择和构造有用特征的过程。包括特征选择、特征变换、特征构造、特征缩放等，对模型性能有重要影响。

## 深度学习

Q6: 什么是反向传播算法？
A6: 反向传播是训练神经网络的核心算法，通过链式法则计算损失函数对各层参数的梯度，从输出层向输入层逐层传播，更新网络权重。

Q7: 卷积神经网络的优势是什么？
A7: CNN具有局部连接、权重共享、平移不变性等特点，特别适合处理图像数据。能够自动学习层次化特征，从边缘到复杂模式，在计算机视觉任务中表现出色。

Q8: 什么是注意力机制？
A8: 注意力机制允许模型在处理序列时动态关注不同位置的信息，解决了RNN在长序列上的信息丢失问题。Transformer架构完全基于注意力机制，在NLP任务中取得突破性进展。

Q9: 什么是迁移学习？
A9: 迁移学习利用在大数据集上预训练的模型，通过微调适应新任务。可以显著减少训练时间和数据需求，特别适用于数据稀缺的场景。

Q10: 什么是生成对抗网络(GAN)？
A10: GAN由生成器和判别器组成，生成器学习生成逼真数据，判别器学习区分真假数据。两者相互对抗训练，最终生成器能产生高质量的合成数据。

## 自然语言处理

Q11: 什么是词嵌入？
A11: 词嵌入将词汇映射到低维稠密向量空间，捕获词汇的语义关系。Word2Vec、GloVe等方法通过上下文信息学习词向量，使语义相似的词在向量空间中距离更近。

Q12: Transformer架构的核心创新是什么？
A12: Transformer的核心创新是自注意力机制和位置编码，完全摒弃了循环和卷积结构。能够并行处理序列，更好地捕获长距离依赖关系，成为现代NLP模型的基础架构。

Q13: 什么是BERT模型？
A13: BERT是基于Transformer的双向编码器，通过掩码语言模型和下一句预测任务进行预训练。能够理解上下文的双向信息，在多项NLP任务中刷新了性能记录。

Q14: 什么是大语言模型？
A14: 大语言模型是参数量巨大的神经网络模型，通过在海量文本数据上预训练获得强大的语言理解和生成能力。GPT、ChatGPT等模型展现了惊人的零样本和少样本学习能力。

Q15: 什么是提示工程？
A15: 提示工程是设计有效输入提示来引导大语言模型产生期望输出的技术。包括零样本提示、少样本提示、思维链提示等方法，是充分发挥大模型能力的关键技术。