#!/usr/bin/env python3
"""
OpenAIå…¼å®¹APIæµ‹è¯•ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨industry_evaluationç³»ç»Ÿæµ‹è¯•ä¸åŒçš„OpenAIå…¼å®¹APIæä¾›å•†ï¼Œ
åŒ…æ‹¬BigModel GLMã€æ™ºè°±AIã€DeepSeekç­‰ã€‚

ä½¿ç”¨æ–¹æ³•:
python examples/openai_compatible_api_test.py --provider bigmodel --api-key YOUR_API_KEY --model glm-4.5
"""

import sys
import os
import argparse
import json
import time
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from industry_evaluation.adapters.model_adapter import ModelAdapterFactory, ModelManager
from industry_evaluation.core.interfaces import EvaluationConfig
from industry_evaluation.models.data_models import EvaluationDimension


def create_test_dataset() -> List[Dict[str, Any]]:
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
    return [
        {
            "id": "test_1",
            "input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç®€è¦ä»‹ç»ã€‚",
            "expected_output": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "context": {
                "domain": "technology",
                "difficulty": "basic"
            }
        },
        {
            "id": "test_2", 
            "input": "ä¸­å›½å¤§æ¨¡å‹è¡Œä¸šåœ¨2025å¹´å°†é¢ä¸´å“ªäº›æœºé‡å’ŒæŒ‘æˆ˜ï¼Ÿ",
            "expected_output": "2025å¹´ä¸­å›½å¤§æ¨¡å‹è¡Œä¸šå°†é¢ä¸´æŠ€æœ¯åˆ›æ–°ã€å¸‚åœºç«äº‰ã€ç›‘ç®¡æ”¿ç­–ç­‰å¤šæ–¹é¢çš„æœºé‡å’ŒæŒ‘æˆ˜ã€‚",
            "context": {
                "domain": "industry_analysis",
                "difficulty": "advanced"
            }
        },
        {
            "id": "test_3",
            "input": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡ã€‚",
            "expected_output": "è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šæ³›åŒ–èƒ½åŠ›å·®çš„ç°è±¡ã€‚",
            "context": {
                "domain": "machine_learning",
                "difficulty": "intermediate"
            }
        }
    ]


def test_model_adapter(provider: str, api_key: str, model_name: str, 
                      base_url: str = None, verbose: bool = False) -> Dict[str, Any]:
    """
    æµ‹è¯•æ¨¡å‹é€‚é…å™¨
    
    Args:
        provider: æä¾›å•†åç§°
        api_key: APIå¯†é’¥
        model_name: æ¨¡å‹åç§°
        base_url: è‡ªå®šä¹‰APIåŸºç¡€URL
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        Dict[str, Any]: æµ‹è¯•ç»“æœ
    """
    print(f"\n=== æµ‹è¯• {provider} æä¾›å•†çš„ {model_name} æ¨¡å‹ ===")
    
    try:
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager()
        
        # å‡†å¤‡é…ç½®
        config_kwargs = {}
        if base_url:
            config_kwargs["base_url"] = base_url
        
        # åˆ›å»ºé€‚é…å™¨
        if provider in ModelAdapterFactory.get_supported_providers():
            # ä½¿ç”¨é¢„å®šä¹‰çš„æä¾›å•†é…ç½®
            adapter = ModelAdapterFactory.create_openai_compatible_adapter(
                model_id=f"{provider}_{model_name}",
                provider=provider,
                api_key=api_key,
                model_name=model_name,
                **config_kwargs
            )
        else:
            # ä½¿ç”¨é€šç”¨OpenAIå…¼å®¹é…ç½®
            config = {
                "api_key": api_key,
                "model_name": model_name,
                "base_url": base_url or "https://api.openai.com/v1",
                "provider": provider,
                **config_kwargs
            }
            adapter = ModelAdapterFactory.create_adapter(
                "openai_compatible", 
                f"{provider}_{model_name}", 
                config
            )
        
        # æ³¨å†Œæ¨¡å‹
        model_manager.register_model(
            f"{provider}_{model_name}",
            "openai_compatible",
            adapter.config
        )
        
        # æµ‹è¯•ç»“æœ
        results = {
            "provider": provider,
            "model_name": model_name,
            "model_id": f"{provider}_{model_name}",
            "tests": [],
            "summary": {
                "total_tests": 0,
                "successful_tests": 0,
                "failed_tests": 0,
                "average_response_time": 0.0
            }
        }
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_dataset = create_test_dataset()
        total_response_time = 0.0
        
        print(f"å¼€å§‹æµ‹è¯• {len(test_dataset)} ä¸ªæ ·æœ¬...")
        
        for i, sample in enumerate(test_dataset, 1):
            print(f"\n--- æµ‹è¯•æ ·æœ¬ {i}/{len(test_dataset)} ---")
            print(f"è¾“å…¥: {sample['input'][:100]}...")
            
            test_result = {
                "sample_id": sample["id"],
                "input": sample["input"],
                "success": False,
                "response": "",
                "response_time": 0.0,
                "error": None
            }
            
            try:
                start_time = time.time()
                
                # è°ƒç”¨æ¨¡å‹
                response = adapter.predict(
                    sample["input"],
                    {
                        "max_tokens": 500,
                        "temperature": 0.7
                    }
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                
                test_result.update({
                    "success": True,
                    "response": response,
                    "response_time": response_time
                })
                
                total_response_time += response_time
                results["summary"]["successful_tests"] += 1
                
                print(f"âœ… æˆåŠŸ (è€—æ—¶: {response_time:.2f}s)")
                if verbose:
                    print(f"å“åº”: {response[:200]}...")
                
            except Exception as e:
                test_result.update({
                    "success": False,
                    "error": str(e)
                })
                
                results["summary"]["failed_tests"] += 1
                print(f"âŒ å¤±è´¥: {str(e)}")
            
            results["tests"].append(test_result)
            results["summary"]["total_tests"] += 1
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(test_dataset):
                time.sleep(1)
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if results["summary"]["successful_tests"] > 0:
            results["summary"]["average_response_time"] = total_response_time / results["summary"]["successful_tests"]
        
        # æ‰“å°æ€»ç»“
        print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
        print(f"æ€»æµ‹è¯•æ•°: {results['summary']['total_tests']}")
        print(f"æˆåŠŸ: {results['summary']['successful_tests']}")
        print(f"å¤±è´¥: {results['summary']['failed_tests']}")
        print(f"æˆåŠŸç‡: {results['summary']['successful_tests']/results['summary']['total_tests']*100:.1f}%")
        if results["summary"]["average_response_time"] > 0:
            print(f"å¹³å‡å“åº”æ—¶é—´: {results['summary']['average_response_time']:.2f}s")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return {
            "provider": provider,
            "model_name": model_name,
            "error": str(e),
            "success": False
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="OpenAIå…¼å®¹APIæµ‹è¯•å·¥å…·")
    parser.add_argument("--provider", required=True, 
                       help="APIæä¾›å•† (bigmodel, zhipu, deepseek, moonshot, openai, æˆ–è‡ªå®šä¹‰)")
    parser.add_argument("--api-key", required=True, help="APIå¯†é’¥")
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--base-url", help="è‡ªå®šä¹‰APIåŸºç¡€URL")
    parser.add_argument("--verbose", "-v", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º")
    parser.add_argument("--output", "-o", help="ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("OpenAIå…¼å®¹APIæµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # æ˜¾ç¤ºæ”¯æŒçš„æä¾›å•†
    supported_providers = ModelAdapterFactory.get_supported_providers()
    print(f"æ”¯æŒçš„é¢„é…ç½®æä¾›å•†: {', '.join(supported_providers)}")
    
    if args.provider in supported_providers:
        print(f"âœ… ä½¿ç”¨é¢„é…ç½®çš„ {args.provider} æä¾›å•†è®¾ç½®")
    else:
        print(f"âš ï¸  ä½¿ç”¨è‡ªå®šä¹‰æä¾›å•† {args.provider}ï¼Œéœ€è¦æä¾› --base-url")
        if not args.base_url:
            print("âŒ è‡ªå®šä¹‰æä¾›å•†éœ€è¦æŒ‡å®š --base-url å‚æ•°")
            return 1
    
    # æ‰§è¡Œæµ‹è¯•
    results = test_model_adapter(
        provider=args.provider,
        api_key=args.api_key,
        model_name=args.model,
        base_url=args.base_url,
        verbose=args.verbose
    )
    
    # ä¿å­˜ç»“æœ
    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    # è¿”å›é€€å‡ºç 
    if results.get("success", True) and results.get("summary", {}).get("failed_tests", 0) == 0:
        return 0
    else:
        return 1


if __name__ == "__main__":
    exit(main())